{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (4.13.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from requests) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from beautifulsoup4) (4.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Google Search Page 1: https://www.google.com/search?q=site%3Alinkedin.com%2Fpulse%2F+%22intelligence+artificielle%22+OR+%22machine+learning%22&start=0\n",
      "Scraping Google Search Page 2: https://www.google.com/search?q=site%3Alinkedin.com%2Fpulse%2F+%22intelligence+artificielle%22+OR+%22machine+learning%22&start=10\n",
      "Scraping Google Search Page 3: https://www.google.com/search?q=site%3Alinkedin.com%2Fpulse%2F+%22intelligence+artificielle%22+OR+%22machine+learning%22&start=20\n",
      "\n",
      "--- Total Unique LinkedIn Pulse URLs Found: 0 ---\n",
      "\n",
      "--- Next Step: Develop strategy to fetch full content from these URLs ---\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import urllib.parse # Pour encoder la requête pour l'URL\n",
    "\n",
    "# --- Configuration ---\n",
    "# Requête de recherche : cible les articles LinkedIn Pulse sur l'IA\n",
    "search_query = 'site:linkedin.com/pulse/ \"intelligence artificielle\" OR \"machine learning\"'\n",
    "# Nombre de pages de résultats à explorer (10 résultats par page environ)\n",
    "num_pages_to_scrape = 3\n",
    "# User agent pour simuler un navigateur\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# --- Stockage des URLs ---\n",
    "found_linkedin_urls = set() # Utilise un set pour éviter les doublons\n",
    "\n",
    "# --- Boucle sur les pages de résultats ---\n",
    "for page in range(num_pages_to_scrape):\n",
    "    start_index = page * 10\n",
    "    # Encoder la requête pour l'URL Google\n",
    "    encoded_query = urllib.parse.quote_plus(search_query)\n",
    "    google_url = f\"https://www.google.com/search?q={encoded_query}&start={start_index}\"\n",
    "\n",
    "    print(f\"Scraping Google Search Page {page + 1}: {google_url}\")\n",
    "\n",
    "    try:\n",
    "        # Faire la requête HTTP\n",
    "        response = requests.get(google_url, headers=headers)\n",
    "        response.raise_for_status() # Vérifier si la requête a réussi\n",
    "\n",
    "        # Analyser le HTML\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Trouver les balises contenant les liens de résultats\n",
    "        # ATTENTION : Les sélecteurs ('div', class_='...) peuvent changer! Inspectez le code source\n",
    "        # de Google pour trouver les bons sélecteurs si cela ne fonctionne pas.\n",
    "        link_tags = soup.find_all('a') # Simple approche : prendre tous les liens\n",
    "\n",
    "        for tag in link_tags:\n",
    "            href = tag.get('href')\n",
    "\n",
    "            # Nettoyer et filtrer les URLs Google pour extraire les liens LinkedIn\n",
    "            if href and href.startswith('/url?q='):\n",
    "                # Extrait l'URL réelle de la redirection Google\n",
    "                clean_url = urllib.parse.unquote(href.split('/url?q=')[1].split('&sa=')[0])\n",
    "\n",
    "                # Vérifier si c'est bien une URL LinkedIn Pulse visée\n",
    "                if 'linkedin.com/pulse/article/' in clean_url:\n",
    "                    found_linkedin_urls.add(clean_url)\n",
    "                    # print(f\"  -> Found potential URL: {clean_url}\") # Décommentez pour voir chaque URL trouvée\n",
    "\n",
    "        # Pause polie pour ne pas surcharger Google\n",
    "        time.sleep(random.uniform(2, 5)) # Attendre entre 2 et 5 secondes\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"  Error during requests to {google_url}: {e}\")\n",
    "        # Possibilité de s'arrêter ou de réessayer ici\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"  An unexpected error occurred: {e}\")\n",
    "\n",
    "# --- Afficher les résultats ---\n",
    "print(f\"\\n--- Total Unique LinkedIn Pulse URLs Found: {len(found_linkedin_urls)} ---\")\n",
    "for url in sorted(list(found_linkedin_urls)):\n",
    "    print(url)\n",
    "\n",
    "# --- ÉTAPE SUIVANTE ---\n",
    "# Maintenant, il faut décider comment obtenir le contenu complet de ces URLs.\n",
    "# Option 1: Visiter manuellement quelques URLs pour évaluer la qualité.\n",
    "# Option 2: Coder la partie scraping de LinkedIn (avec prudence).\n",
    "# Option 3: Chercher si ces articles existent sur d'autres plateformes plus faciles à scraper.\n",
    "print(\"\\n--- Next Step: Develop strategy to fetch full content from these URLs ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googlesearch-python\n",
      "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from googlesearch-python) (4.13.3)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from googlesearch-python) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from beautifulsoup4>=4.9->googlesearch-python) (4.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from requests>=2.20->googlesearch-python) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from requests>=2.20->googlesearch-python) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from requests>=2.20->googlesearch-python) (2025.1.31)\n",
      "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
      "Installing collected packages: googlesearch-python\n",
      "Successfully installed googlesearch-python-1.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install googlesearch-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recherche Google en cours pour : 'site:linkedin.com/pulse/ \"intelligence artificielle\" OR \"machine learning\"'\n",
      "Tentative de récupérer jusqu'à 50 résultats...\n",
      "\n",
      "Une erreur est survenue pendant la recherche Google : search() got an unexpected keyword argument 'num'\n",
      "Cela peut être dû à un blocage temporaire de Google.\n",
      "\n",
      "--- Total Unique LinkedIn Pulse URLs Found: 0 ---\n",
      "Aucune URL correspondante trouvée. Essayez d'ajuster la requête de recherche ('search_query').\n",
      "\n",
      "--- Next Step: Evaluate the found URLs and develop strategy to fetch full content ---\n"
     ]
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "# Requête de recherche : cible les articles LinkedIn Pulse sur l'IA\n",
    "search_query = 'site:linkedin.com/pulse/ \"intelligence artificielle\" OR \"machine learning\"'\n",
    "# Nombre total de résultats souhaités\n",
    "num_results = 50 # Essayons d'en obtenir 50 (la bibliothèque gère la pagination)\n",
    "# Délai entre les requêtes à Google (en secondes) pour être poli\n",
    "pause_duration = 3.0\n",
    "\n",
    "# --- Stockage des URLs ---\n",
    "found_linkedin_urls = set() # Utilise un set pour éviter les doublons\n",
    "\n",
    "print(f\"Recherche Google en cours pour : '{search_query}'\")\n",
    "print(f\"Tentative de récupérer jusqu'à {num_results} résultats...\")\n",
    "\n",
    "try:\n",
    "    # Utilisation de la fonction search de la bibliothèque\n",
    "    # num : nombre de résultats à récupérer par requête interne\n",
    "    # stop : nombre total de résultats à récupérer\n",
    "    # pause : délai entre les requêtes HTTP\n",
    "    for url in search(search_query, num=10, stop=num_results, pause=pause_duration):\n",
    "        # Filtrer pour ne garder que les URLs LinkedIn Pulse pertinentes\n",
    "        # Adaptez si vous ciblez autre chose que /pulse/article/\n",
    "        if 'linkedin.com/pulse/article/' in url:\n",
    "            if url not in found_linkedin_urls:\n",
    "              print(f\"  -> Trouvé : {url}\")\n",
    "              found_linkedin_urls.add(url)\n",
    "        # Petite pause supplémentaire pour être sûr (optionnel)\n",
    "        # time.sleep(0.5)\n",
    "\n",
    "except Exception as e:\n",
    "    # La bibliothèque peut parfois lever des erreurs si Google bloque\n",
    "    print(f\"\\nUne erreur est survenue pendant la recherche Google : {e}\")\n",
    "    print(\"Cela peut être dû à un blocage temporaire de Google.\")\n",
    "\n",
    "# --- Afficher les résultats ---\n",
    "print(f\"\\n--- Total Unique LinkedIn Pulse URLs Found: {len(found_linkedin_urls)} ---\")\n",
    "if not found_linkedin_urls:\n",
    "    print(\"Aucune URL correspondante trouvée. Essayez d'ajuster la requête de recherche ('search_query').\")\n",
    "else:\n",
    "    # Optionnel : Sauvegarder les URLs dans un fichier\n",
    "    try:\n",
    "        with open(\"linkedin_pulse_urls.txt\", \"w\") as f:\n",
    "            for url in sorted(list(found_linkedin_urls)):\n",
    "                f.write(url + \"\\n\")\n",
    "        print(\"Les URLs trouvées ont été sauvegardées dans 'linkedin_pulse_urls.txt'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la sauvegarde des URLs : {e}\")\n",
    "\n",
    "\n",
    "# --- ÉTAPE SUIVANTE ---\n",
    "print(\"\\n--- Next Step: Evaluate the found URLs and develop strategy to fetch full content ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan de Collecte et Préparation des Données pour le Nouveau Fine-Tuning IA\n",
    "\n",
    "**Objectif :** Créer un jeu de données de haute qualité composé de posts LinkedIn pertinents et engageants sur l'Intelligence Artificielle, afin d'affiner un modèle GPT plus performant et flexible dans ce domaine.\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 1 : Découverte des URLs via Google Search\n",
    "\n",
    "* **But :** Identifier un maximum d'URLs de posts ou d'articles LinkedIn pertinents sur divers sujets liés à l'IA (IA générale, IA générative, Acte IA, actus, etc.).\n",
    "* **Méthode :** Utilisation d'un script Python avec la bibliothèque `googlesearch-python` pour interroger Google Search avec des mots-clés et des opérateurs `site:linkedin.com`.\n",
    "* **Entrée :** Requêtes de recherche Google larges couvrant les thèmes IA visés.\n",
    "* **Sortie :** Un fichier texte (`linkedin_urls_found.txt`) contenant une liste d'URLs LinkedIn potentiellement intéressantes.\n",
    "* **Code :** Premier script dans ce notebook (`scraping_preparation.ipynb`).\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 2 : Scraping du Contenu et de l'Engagement sur LinkedIn\n",
    "\n",
    "* **But :** Pour chaque URL identifiée en Phase 1, récupérer le texte complet du post/article ainsi que ses métriques d'engagement (nombre de likes, de commentaires).\n",
    "* **Méthode :** Développer un script Python pour visiter chaque URL LinkedIn. Utilisation de techniques de scraping web (ex: `requests` + `BeautifulSoup4`, ou potentiellement `Selenium`/`Playwright` si nécessaire) pour analyser le HTML de la page LinkedIn et extraire les informations désirées.\n",
    "* **Entrée :** Le fichier `linkedin_urls_found.txt` de la Phase 1.\n",
    "* **Sortie :** Un fichier structuré (ex: CSV, JSON) contenant les URLs, le texte des posts, et les nombres de likes/commentaires.\n",
    "* **⚠️ Attention :** Phase techniquement complexe, potentiellement fragile (changements de structure LinkedIn), et nécessitant de la prudence vis-à-vis des conditions d'utilisation de LinkedIn.\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 3 : Filtrage des Données et Formatage pour OpenAI\n",
    "\n",
    "* **But :** Sélectionner les posts les plus \"pertinents\" (basé sur l'engagement) et les formater correctement pour l'API de fine-tuning d'OpenAI.\n",
    "* **Méthode :**\n",
    "    1.  Analyser les données de la Phase 2 pour définir un seuil d'engagement (ex: garder les posts avec > N likes/commentaires).\n",
    "    2.  Filtrer le jeu de données pour ne conserver que ces posts \"performants\".\n",
    "    3.  Convertir le contenu textuel sélectionné au format JSONL attendu par OpenAI (généralement une structure de messages `{\"messages\": [{\"role\": \"system\", ...}, {\"role\": \"user\", ...}, {\"role\": \"assistant\", \"content\": \"TEXTE_DU_POST\"}]}`).\n",
    "* **Entrée :** Les données structurées de la Phase 2.\n",
    "* **Sortie :** Le fichier final `fine-tuning-data-ai.jsonl` (ou nom similaire) prêt à être uploadé sur OpenAI pour le nouveau job de fine-tuning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recherche Google élargie en cours pour : 'site:linkedin.com/pulse/ (\"intelligence artificielle\" OR \"IA générative\" OR \"generative AI\" OR \"EU AI Act\" OR \"Acte IA Europe\" OR \"actualité IA\" OR \"machine learning\") OR site:linkedin.com/posts/ (\"intelligence artificielle\" OR \"IA générative\" OR \"generative AI\" OR \"EU AI Act\" OR \"Acte IA Europe\" OR \"actualité IA\" OR \"machine learning\")'\n",
      "Tentative de récupérer jusqu'à 100 résultats...\n",
      "\n",
      "Une erreur est survenue pendant la recherche Google : search() got an unexpected keyword argument 'stop'\n",
      "Cela peut être dû à un blocage temporaire de Google ou à un autre problème.\n",
      "\n",
      "--- Total Unique LinkedIn URLs Found: 0 ---\n",
      "Aucune URL correspondante trouvée. Essayez d'ajuster ou de simplifier la requête de recherche ('search_query'), ou augmentez 'num_results'.\n",
      "\n",
      "--- Prochaine Étape (Phase 2) : Développer le code pour visiter les 0 URLs trouvées, ---\n",
      "--- extraire le contenu complet des posts ET les données d'engagement (likes/commentaires). ---\n",
      "--- Attention : Cette étape nécessitera du scraping direct de LinkedIn et sera plus complexe. ---\n"
     ]
    }
   ],
   "source": [
    "# Étape 1 : Installer la bibliothèque si ce n'est pas fait\n",
    "# !pip install googlesearch-python\n",
    "\n",
    "from googlesearch import search\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "# Requête de recherche élargie pour divers sujets IA sur LinkedIn\n",
    "search_query = (\n",
    "    'site:linkedin.com/pulse/ (\"intelligence artificielle\" OR \"IA générative\" OR \"generative AI\" OR \"EU AI Act\" OR \"Acte IA Europe\" OR \"actualité IA\" OR \"machine learning\") '\n",
    "    'OR site:linkedin.com/posts/ (\"intelligence artificielle\" OR \"IA générative\" OR \"generative AI\" OR \"EU AI Act\" OR \"Acte IA Europe\" OR \"actualité IA\" OR \"machine learning\")'\n",
    ")\n",
    "# Nombre total de résultats souhaités (ajustez si nécessaire)\n",
    "num_results = 100 # Essayons d'en obtenir 100\n",
    "# Délai entre les requêtes à Google (en secondes) - Important !\n",
    "pause_duration = 4.0 # Augmenté à 4 secondes pour être plus prudent\n",
    "\n",
    "# --- Stockage des URLs ---\n",
    "found_linkedin_urls = set()\n",
    "\n",
    "print(f\"Recherche Google élargie en cours pour : '{search_query}'\")\n",
    "print(f\"Tentative de récupérer jusqu'à {num_results} résultats...\")\n",
    "\n",
    "try:\n",
    "    # Utilisation de la fonction search corrigée (num_results au lieu de num)\n",
    "    for url in search(search_query, num_results=10, stop=num_results, pause=pause_duration):\n",
    "        # Filtrer pour ne garder que les URLs LinkedIn (Pulse ou Posts)\n",
    "        if 'linkedin.com/' in url:\n",
    "            if '/groups/' not in url and '/jobs/' not in url and '/company/' not in url: # Exclure certains types\n",
    "                if url not in found_linkedin_urls:\n",
    "                  print(f\"  -> Trouvé : {url}\")\n",
    "                  found_linkedin_urls.add(url)\n",
    "        # time.sleep(0.2) # Petite pause supplémentaire (optionnel)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nUne erreur est survenue pendant la recherche Google : {e}\")\n",
    "    print(\"Cela peut être dû à un blocage temporaire de Google ou à un autre problème.\")\n",
    "\n",
    "# --- Afficher les résultats ---\n",
    "print(f\"\\n--- Total Unique LinkedIn URLs Found: {len(found_linkedin_urls)} ---\")\n",
    "if not found_linkedin_urls:\n",
    "    print(\"Aucune URL correspondante trouvée. Essayez d'ajuster ou de simplifier la requête de recherche ('search_query'), ou augmentez 'num_results'.\")\n",
    "else:\n",
    "    # Sauvegarder les URLs dans un fichier\n",
    "    output_filename = \"linkedin_urls_found.txt\" # Sauvegardé dans le dossier 01_collecte_donnees\n",
    "    try:\n",
    "        with open(output_filename, \"w\") as f:\n",
    "            for url in sorted(list(found_linkedin_urls)):\n",
    "                f.write(url + \"\\n\")\n",
    "        print(f\"Les URLs trouvées ont été sauvegardées dans '{output_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la sauvegarde des URLs : {e}\")\n",
    "\n",
    "\n",
    "# --- Phase 2 (Code Suivant) ---\n",
    "print(f\"\\n--- Prochaine Étape (Phase 2) : Développer le code pour visiter les {len(found_linkedin_urls)} URLs trouvées, ---\")\n",
    "print(\"--- extraire le contenu complet des posts ET les données d'engagement (likes/commentaires). ---\")\n",
    "print(\"--- Attention : Cette étape nécessitera du scraping direct de LinkedIn et sera plus complexe. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recherche Google élargie en cours pour : 'site:linkedin.com/pulse/ (\"intelligence artificielle\" OR \"IA générative\" OR \"generative AI\" OR \"EU AI Act\" OR \"Acte IA Europe\" OR \"actualité IA\" OR \"machine learning\") OR site:linkedin.com/posts/ (\"intelligence artificielle\" OR \"IA générative\" OR \"generative AI\" OR \"EU AI Act\" OR \"Acte IA Europe\" OR \"actualité IA\" OR \"machine learning\")'\n",
      "Traitement d'au maximum 100 résultats Google...\n",
      "\n",
      "Une erreur est survenue pendant la recherche Google : search() got an unexpected keyword argument 'pause'\n",
      "Cela peut être dû à un blocage temporaire de Google ou à un autre problème.\n",
      "\n",
      "--- Total Unique LinkedIn URLs Found: 0 ---\n",
      "Aucune URL LinkedIn pertinente trouvée parmi les 0 premiers résultats Google.\n",
      "Essayez d'ajuster ou de simplifier la requête de recherche ('search_query').\n",
      "\n",
      "--- Prochaine Étape (Phase 2) : Développer le code pour visiter les 0 URLs trouvées, ---\n",
      "--- extraire le contenu complet des posts ET les données d'engagement (likes/commentaires). ---\n",
      "--- Attention : Cette étape nécessitera du scraping direct de LinkedIn et sera plus complexe. ---\n"
     ]
    }
   ],
   "source": [
    "# Étape 1 : Installer la bibliothèque si ce n'est pas fait\n",
    "# !pip install googlesearch-python\n",
    "\n",
    "from googlesearch import search\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "search_query = (\n",
    "    'site:linkedin.com/pulse/ (\"intelligence artificielle\" OR \"IA générative\" OR \"generative AI\" OR \"EU AI Act\" OR \"Acte IA Europe\" OR \"actualité IA\" OR \"machine learning\") '\n",
    "    'OR site:linkedin.com/posts/ (\"intelligence artificielle\" OR \"IA générative\" OR \"generative AI\" OR \"EU AI Act\" OR \"Acte IA Europe\" OR \"actualité IA\" OR \"machine learning\")'\n",
    ")\n",
    "# Limite manuelle du nombre de résultats Google à traiter\n",
    "results_limit = 100\n",
    "# Délai entre les requêtes à Google (en secondes) - Important !\n",
    "pause_duration = 4.0\n",
    "\n",
    "# --- Stockage des URLs ---\n",
    "found_linkedin_urls = set()\n",
    "processed_google_results = 0 # Compteur manuel\n",
    "\n",
    "print(f\"Recherche Google élargie en cours pour : '{search_query}'\")\n",
    "print(f\"Traitement d'au maximum {results_limit} résultats Google...\")\n",
    "\n",
    "try:\n",
    "    # Appel simplifié de la fonction search (uniquement query et pause)\n",
    "    for url in search(search_query, pause=pause_duration):\n",
    "        processed_google_results += 1\n",
    "        # Filtrer pour ne garder que les URLs LinkedIn pertinentes\n",
    "        if 'linkedin.com/' in url:\n",
    "            if '/groups/' not in url and '/jobs/' not in url and '/company/' not in url: # Exclure certains types\n",
    "                if url not in found_linkedin_urls:\n",
    "                  print(f\"  -> Trouvé ({processed_google_results}): {url}\")\n",
    "                  found_linkedin_urls.add(url)\n",
    "\n",
    "        # Arrêter manuellement si on a traité assez de résultats Google\n",
    "        if processed_google_results >= results_limit:\n",
    "            print(f\"\\nLimite de {results_limit} résultats Google traités atteinte.\")\n",
    "            break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nUne erreur est survenue pendant la recherche Google : {e}\")\n",
    "    print(\"Cela peut être dû à un blocage temporaire de Google ou à un autre problème.\")\n",
    "\n",
    "# --- Afficher les résultats ---\n",
    "print(f\"\\n--- Total Unique LinkedIn URLs Found: {len(found_linkedin_urls)} ---\")\n",
    "if not found_linkedin_urls:\n",
    "    print(f\"Aucune URL LinkedIn pertinente trouvée parmi les {processed_google_results} premiers résultats Google.\")\n",
    "    print(\"Essayez d'ajuster ou de simplifier la requête de recherche ('search_query').\")\n",
    "else:\n",
    "    # Sauvegarder les URLs dans un fichier\n",
    "    output_filename = \"linkedin_urls_found.txt\"\n",
    "    try:\n",
    "        with open(output_filename, \"w\") as f:\n",
    "            for url in sorted(list(found_linkedin_urls)):\n",
    "                f.write(url + \"\\n\")\n",
    "        print(f\"Les URLs trouvées ont été sauvegardées dans '{output_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la sauvegarde des URLs : {e}\")\n",
    "\n",
    "\n",
    "# --- Phase 2 (Code Suivant) ---\n",
    "print(f\"\\n--- Prochaine Étape (Phase 2) : Développer le code pour visiter les {len(found_linkedin_urls)} URLs trouvées, ---\")\n",
    "print(\"--- extraire le contenu complet des posts ET les données d'engagement (likes/commentaires). ---\")\n",
    "print(\"--- Attention : Cette étape nécessitera du scraping direct de LinkedIn et sera plus complexe. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recherche Google élargie en cours pour : 'site:linkedin.com/pulse/ (\"intelligence artificielle\" OR \"IA générative\" OR \"generative AI\" OR \"EU AI Act\" OR \"Acte IA Europe\" OR \"actualité IA\" OR \"machine learning\") OR site:linkedin.com/posts/ (\"intelligence artificielle\" OR \"IA générative\" OR \"generative AI\" OR \"EU AI Act\" OR \"Acte IA Europe\" OR \"actualité IA\" OR \"machine learning\")'\n",
      "Tentative avec appel EXTRÊMEMENT simplifié (sans pause/stop/num)...\n",
      "  -> Trouvé (1): https://fr.linkedin.com/pulse/intelligence-artificielle-g%C3%A9n%C3%A9rative-r%C3%A9volution-et-applications-urdhe\n",
      "  -> Trouvé (2): https://fr.linkedin.com/pulse/machine-learning-llm-ia-g%C3%A9n%C3%A9rative-ce-que-ces-types-dia-ribeiro-aiyff\n",
      "  -> Trouvé (3): https://fr.linkedin.com/pulse/lia-g%C3%A9n%C3%A9rative-r%C3%A9volution-ou-simple-%C3%A9volution-maltem-africa-2ahae\n",
      "  -> Trouvé (4): https://fr.linkedin.com/pulse/lintelligence-artificielle-g%C3%A9n%C3%A9rative-un-nouveau-n8q6e\n",
      "  -> Trouvé (5): https://fr.linkedin.com/pulse/lai-act-encadrer-lintelligence-artificielle-pour-un-futur-spitz-xks0e\n",
      "  -> Trouvé (6): https://fr.linkedin.com/pulse/machine-learning-ia-vs-g%C3%A9n%C3%A9rative-comprendre-lessentiel-sarr-mtcne\n",
      "  -> Trouvé (7): https://fr.linkedin.com/pulse/ia-g%C3%A9n%C3%A9rative-les-d%C3%A9fis-r%C3%A9glementaires-%C3%A9thiques-et-pour-manchau-phdge\n",
      "  -> Trouvé (8): https://fr.linkedin.com/pulse/intelligence-artificielle-ia-g%C3%A9n%C3%A9rative-%C3%A0-la-chatgpt-et-bughin\n",
      "  -> Trouvé (9): https://fr.linkedin.com/pulse/derni%C3%A8res-avanc%C3%A9es-en-ia-g%C3%A9n%C3%A9rative-mars-2025-arnault-chatel-zs39e\n",
      "  -> Trouvé (10): https://fr.linkedin.com/pulse/comment-arbitrer-entre-ia-classique-et-g%C3%A9n%C3%A9rative-le-de-mascureau-2drce\n",
      "\n",
      "--- Total Unique LinkedIn URLs Found: 10 (10 résultats Google traités) ---\n",
      "Les URLs trouvées ont été sauvegardées dans 'linkedin_urls_found.txt'\n",
      "\n",
      "--- Prochaine Étape (Phase 2) : Développer le code pour visiter les 10 URLs trouvées, ---\n",
      "--- extraire le contenu complet des posts ET les données d'engagement (likes/commentaires). ---\n",
      "--- Attention : Cette étape nécessitera du scraping direct de LinkedIn et sera plus complexe. ---\n"
     ]
    }
   ],
   "source": [
    "# Étape 1 : Installer la bibliothèque si ce n'est pas fait\n",
    "# !pip install googlesearch-python\n",
    "\n",
    "from googlesearch import search\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "search_query = (\n",
    "    'site:linkedin.com/pulse/ (\"intelligence artificielle\" OR \"IA générative\" OR \"generative AI\" OR \"EU AI Act\" OR \"Acte IA Europe\" OR \"actualité IA\" OR \"machine learning\") '\n",
    "    'OR site:linkedin.com/posts/ (\"intelligence artificielle\" OR \"IA générative\" OR \"generative AI\" OR \"EU AI Act\" OR \"Acte IA Europe\" OR \"actualité IA\" OR \"machine learning\")'\n",
    ")\n",
    "# On ne contrôle plus la limite via les arguments\n",
    "print(f\"Recherche Google élargie en cours pour : '{search_query}'\")\n",
    "print(\"Tentative avec appel EXTRÊMEMENT simplifié (sans pause/stop/num)...\")\n",
    "\n",
    "# --- Stockage des URLs ---\n",
    "found_linkedin_urls = set()\n",
    "processed_google_results = 0\n",
    "\n",
    "try:\n",
    "    # APPEL EXTRÊMEMENT SIMPLIFIÉ - Uniquement l'argument query\n",
    "    for url in search(search_query):\n",
    "        processed_google_results += 1\n",
    "        # Filtrer pour ne garder que les URLs LinkedIn pertinentes\n",
    "        if 'linkedin.com/' in url:\n",
    "            if '/groups/' not in url and '/jobs/' not in url and '/company/' not in url:\n",
    "                if url not in found_linkedin_urls:\n",
    "                  print(f\"  -> Trouvé ({processed_google_results}): {url}\")\n",
    "                  found_linkedin_urls.add(url)\n",
    "        # On ne peut pas contrôler la pause entre les requêtes Google internes ici\n",
    "        # Une pause ici ne fait que ralentir le traitement des résultats déjà reçus\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    # NOTE: On ne sait pas combien de résultats Google la bibliothèque a réellement tenté de récupérer\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nUne erreur est survenue pendant la recherche Google : {e}\")\n",
    "    print(\"Cela peut être dû à un blocage temporaire de Google ou à un problème avec la bibliothèque.\")\n",
    "    # Regardez si le message d'erreur donne des indices sur les arguments acceptés\n",
    "\n",
    "# --- Afficher les résultats ---\n",
    "print(f\"\\n--- Total Unique LinkedIn URLs Found: {len(found_linkedin_urls)} ({processed_google_results} résultats Google traités) ---\")\n",
    "if not found_linkedin_urls:\n",
    "    print(f\"Aucune URL LinkedIn pertinente trouvée.\")\n",
    "    print(\"La bibliothèque ne retourne peut-être pas de résultats ou le filtrage est trop strict.\")\n",
    "else:\n",
    "    # Sauvegarder les URLs dans un fichier\n",
    "    output_filename = \"linkedin_urls_found.txt\"\n",
    "    try:\n",
    "        with open(output_filename, \"w\") as f:\n",
    "            for url in sorted(list(found_linkedin_urls)):\n",
    "                f.write(url + \"\\n\")\n",
    "        print(f\"Les URLs trouvées ont été sauvegardées dans '{output_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la sauvegarde des URLs : {e}\")\n",
    "\n",
    "\n",
    "# --- Phase 2 (Code Suivant) ---\n",
    "print(f\"\\n--- Prochaine Étape (Phase 2) : Développer le code pour visiter les {len(found_linkedin_urls)} URLs trouvées, ---\")\n",
    "print(\"--- extraire le contenu complet des posts ET les données d'engagement (likes/commentaires). ---\")\n",
    "print(\"--- Attention : Cette étape nécessitera du scraping direct de LinkedIn et sera plus complexe. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approche pour le Développement de la Phase 2 : Scraping LinkedIn\n",
    "\n",
    "**Contexte :** La Phase 1 nous a permis de collecter une première liste d'URLs LinkedIn potentielles via Google Search. La Phase 2 vise à visiter ces URLs pour extraire le contenu texte complet et les métriques d'engagement (likes, commentaires). Cette étape de scraping direct de LinkedIn est connue pour être complexe et sensible aux changements de structure du site.\n",
    "\n",
    "**Stratégie Adoptée : Développement Incrémental**\n",
    "\n",
    "1.  **Tester sur un Petit Échantillon :** Nous allons développer et déboguer le code Python de la Phase 2 en utilisant **uniquement le petit nombre d'URLs (~10) collectées lors du premier test de la Phase 1**.\n",
    "2.  **Valider la Faisabilité :** L'objectif est de prouver qu'il est possible d'extraire les informations souhaitées (texte, likes, commentaires) pour cet échantillon réduit.\n",
    "3.  **Itérer Rapidement :** Travailler avec peu d'URLs permet de tester les changements de code rapidement et de comprendre les difficultés spécifiques du scraping de LinkedIn sans lancer de nombreuses requêtes potentiellement bloquantes.\n",
    "\n",
    "**Avantages de cette Approche :**\n",
    "\n",
    "* Réduit le temps de débogage.\n",
    "* Minimise le risque de blocage par LinkedIn pendant la phase de développement.\n",
    "* Permet de valider le concept avant d'investir du temps dans la collecte d'une grande quantité d'URLs (Phase 1 à grande échelle).\n",
    "\n",
    "**Plan Post-Validation :**\n",
    "\n",
    "* Une fois que le script de la Phase 2 fonctionnera de manière fiable sur le petit échantillon :\n",
    "    * **Améliorer la Phase 1 :** Optimiser ou relancer la collecte d'URLs pour obtenir une liste beaucoup plus conséquente (plusieurs centaines).\n",
    "    * **Exécuter Phase 2 à l'Échelle :** Lancer le script de scraping validé sur la grande liste d'URLs (avec les précautions nécessaires : pauses, gestion des erreurs, etc.).\n",
    "    * **Procéder à la Phase 3 :** Filtrer et formater les données collectées en masse.\n",
    "\n",
    "**Action Immédiate :** Nous commençons maintenant le développement du code de la Phase 2 en ciblant les URLs présentes dans `linkedin_urls_found.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 10 URLs chargées depuis 'linkedin_urls_found.txt'\n",
      " ciblant l'URL : https://fr.linkedin.com/pulse/derni%C3%A8res-avanc%C3%A9es-en-ia-g%C3%A9n%C3%A9rative-mars-2025-arnault-chatel-zs39e\n",
      "\n",
      "Tentative de téléchargement de la page LinkedIn...\n",
      "✅ Succès ! Code de statut : 200\n",
      "\n",
      "--- Extrait du début du code HTML reçu (500 caractères) ---\n",
      "<!DOCTYPE html>\n",
      "\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    \n",
      "    <html class=\"cls-fix-enabled\" lang=\"fr\">\n",
      "      <head>\n",
      "        <meta name=\"pageKey\" content=\"d_flagship2_pulse_read\">\n",
      "          \n",
      "    <meta name=\"robots\" content=\"max-image-preview:large, noarchive\">\n",
      "    <meta name=\"bingbot\" content=\"nocache\">\n",
      "  \n",
      "<!----><!---->        <meta name=\"locale\" content=\"fr_FR\">\n",
      "        <meta id=\"config\" data-app-version=\"0.0.4381\" data-call-tree-id=\"AAYzcHtwPgrBQqXYtCd8cQ==\" data-multiproduct-name=\"\n",
      "--- Fin de l'extrait ---\n",
      "\n",
      "➡️ Prochaine étape : Analyser ce code HTML avec BeautifulSoup pour trouver le contenu.\n",
      "\n",
      "✅ HTML analysé avec BeautifulSoup (initialisation).\n",
      "⚠️ Corps de l'article non trouvé avec le sélecteur actuel.\n",
      "   Veuillez inspecter la page CIBLE dans votre navigateur et adapter le sélecteur ('find').\n",
      "⚠️ Likes/Réactions non trouvés avec le sélecteur actuel.\n",
      "   Veuillez inspecter la page CIBLE et adapter le sélecteur.\n",
      "⚠️ Commentaires non trouvés avec le sélecteur actuel.\n",
      "   Veuillez inspecter la page CIBLE et adapter le sélecteur.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup # On l'importe déjà pour la suite\n",
    "import time\n",
    "import random\n",
    "import os # Pour vérifier l'existence du fichier\n",
    "\n",
    "# --- Configuration ---\n",
    "urls_filename = \"linkedin_urls_found.txt\" # Fichier créé par le script de Phase 1\n",
    "target_url = None # Contiendra l'URL à tester\n",
    "\n",
    "# --- 1. Charger les URLs depuis le fichier ---\n",
    "linkedin_urls = []\n",
    "# Vérifier si le fichier existe dans le dossier courant (01_collecte_donnees)\n",
    "if os.path.exists(urls_filename):\n",
    "    try:\n",
    "        with open(urls_filename, \"r\") as f:\n",
    "            # Lire chaque ligne, enlever les espaces/sauts de ligne, ignorer lignes vides\n",
    "            linkedin_urls = [line.strip() for line in f if line.strip()]\n",
    "        if len(linkedin_urls) >= 2: # Vérifier qu'il y a au moins 2 URLs\n",
    "            print(f\"✅ {len(linkedin_urls)} URLs chargées depuis '{urls_filename}'\")\n",
    "            # ---- MODIFICATION IMPORTANTE ICI ----\n",
    "            target_url = linkedin_urls[1] # Sélectionner la DEUXIÈME URL pour ce test\n",
    "            # ------------------------------------\n",
    "            print(f\" ciblant l'URL : {target_url}\")\n",
    "        elif linkedin_urls:\n",
    "             print(f\"⚠️ Fichier '{urls_filename}' contient seulement {len(linkedin_urls)} URL(s). Impossible de tester la deuxième.\")\n",
    "        else:\n",
    "            print(f\"⚠️ Le fichier '{urls_filename}' est vide. Impossible de continuer.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors de la lecture du fichier '{urls_filename}': {e}\")\n",
    "else:\n",
    "    print(f\"❌ Erreur : Le fichier '{urls_filename}' n'a pas été trouvé.\")\n",
    "    print(\"   Assurez-vous que le script de la Phase 1 a bien fonctionné et créé le fichier.\")\n",
    "\n",
    "# --- 2. Essayer de télécharger la page de l'URL cible (si une URL a été chargée) ---\n",
    "if target_url:\n",
    "    print(\"\\nTentative de téléchargement de la page LinkedIn...\")\n",
    "    # Simuler un navigateur commun pour éviter blocage immédiat\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
    "        'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7' # Préférer le français\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Envoyer la requête GET avec un timeout de 15 secondes\n",
    "        response = requests.get(target_url, headers=headers, timeout=15)\n",
    "\n",
    "        # Vérifier si la requête a échoué (code 4xx ou 5xx)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        print(f\"✅ Succès ! Code de statut : {response.status_code}\")\n",
    "        print(f\"\\n--- Extrait du début du code HTML reçu (500 caractères) ---\")\n",
    "        # Afficher le début du HTML pour voir si ça ressemble à une page LinkedIn ou une page d'erreur/login\n",
    "        print(response.text[:500])\n",
    "        print(\"--- Fin de l'extrait ---\")\n",
    "\n",
    "        # --- Prochaine étape : Analyser le HTML avec BeautifulSoup ---\n",
    "        print(\"\\n➡️ Prochaine étape : Analyser ce code HTML avec BeautifulSoup pour trouver le contenu.\")\n",
    "\n",
    "        # Analyser le HTML avec BeautifulSoup (Partie à activer et adapter après inspection)\n",
    "        try:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            print(\"\\n✅ HTML analysé avec BeautifulSoup (initialisation).\")\n",
    "\n",
    "            # --- Extraction du Contenu de l'Article ---\n",
    "            # !! REMPLACEZ 'div' et 'class_' par ce que vous avez trouvé lors de l'inspection !!\n",
    "            article_body_tag = soup.find('div', class_='classe-du-corps-article-a-trouver') # EXEMPLE, À CHANGER\n",
    "            if article_body_tag:\n",
    "                article_text = article_body_tag.get_text(separator='\\n', strip=True)\n",
    "                print(\"\\n--- Texte de l'Article (Début) ---\")\n",
    "                print(article_text[:1000]) # Afficher les 1000 premiers caractères\n",
    "                print(\"--- Fin de l'extrait ---\")\n",
    "            else:\n",
    "                print(\"⚠️ Corps de l'article non trouvé avec le sélecteur actuel.\")\n",
    "                print(\"   Veuillez inspecter la page CIBLE dans votre navigateur et adapter le sélecteur ('find').\")\n",
    "\n",
    "            # --- Extraction des Likes/Réactions ---\n",
    "            # !! REMPLACEZ le sélecteur par ce que vous avez trouvé !!\n",
    "            likes_tag = soup.find('span', class_='classe-pour-les-likes-a-trouver') # EXEMPLE, À CHANGER\n",
    "            if likes_tag:\n",
    "                likes_text = likes_tag.get_text(strip=True).split()[0]\n",
    "                print(f\"\\n--- Likes/Réactions trouvés : {likes_text}\")\n",
    "            else:\n",
    "                print(\"⚠️ Likes/Réactions non trouvés avec le sélecteur actuel.\")\n",
    "                print(\"   Veuillez inspecter la page CIBLE et adapter le sélecteur.\")\n",
    "\n",
    "\n",
    "            # --- Extraction du Nombre de Commentaires ---\n",
    "            # !! REMPLACEZ le sélecteur par ce que vous avez trouvé !!\n",
    "            comments_tag = soup.find('a', class_='classe-pour-commentaires-a-trouver') # EXEMPLE, À CHANGER\n",
    "            if comments_tag:\n",
    "                comments_text = comments_tag.get_text(strip=True).split()[0]\n",
    "                print(f\"\\n--- Commentaires trouvés : {comments_text}\")\n",
    "            else:\n",
    "                print(\"⚠️ Commentaires non trouvés avec le sélecteur actuel.\")\n",
    "                print(\"   Veuillez inspecter la page CIBLE et adapter le sélecteur.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur pendant l'analyse BeautifulSoup : {e}\")\n",
    "        # --- Fin de l'analyse BeautifulSoup ---\n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"❌ Erreur HTTP : {http_err} - Code de statut : {response.status_code}\")\n",
    "        print(\"   LinkedIn a probablement bloqué la requête ou la page nécessite une connexion.\")\n",
    "        print(f\"\\n--- Extrait de la page d'erreur reçue (500 caractères) ---\")\n",
    "        print(response.text[:500])\n",
    "        print(\"--- Fin de l'extrait ---\")\n",
    "    except requests.exceptions.ConnectionError as conn_err:\n",
    "        print(f\"❌ Erreur de Connexion : {conn_err}\")\n",
    "    except requests.exceptions.Timeout as timeout_err:\n",
    "        print(f\"❌ Délai d'attente dépassé : {timeout_err}\")\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        print(f\"❌ Erreur pendant la requête : {req_err}\")\n",
    "    except Exception as e:\n",
    "         print(f\"❌ Une erreur inattendue est survenue : {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nAucune URL cible sélectionnée ou fichier d'URLs invalide, arrêt.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2 : Validation du Téléchargement HTML (Test sur URL spécifique)\n",
    "\n",
    "**Constat Initial :**\n",
    "La première URL (`index 0`) extraite par le script de la Phase 1 s'est révélée invalide lors d'une visite manuelle dans le navigateur (page \"Article non trouvé\" puis redirection). Le script Python obtenait bien un code de statut 200, mais pour cette page d'erreur, et non pour l'article réel.\n",
    "\n",
    "**Action Corrective :**\n",
    "Pour vérifier si le téléchargement HTML via `requests` était possible sur une URL valide, nous avons :\n",
    "1.  Vérifié manuellement la **deuxième URL** (`index 1`) du fichier `linkedin_urls_found.txt`. Cette URL menait bien à un article existant (\"Les dernières avancées en IA générative...\").\n",
    "2.  Modifié le script Python pour cibler spécifiquement cette **deuxième URL** (`target_url = linkedin_urls[1]`).\n",
    "3.  Ré-exécuté le script.\n",
    "\n",
    "**Résultat Obtenu :**\n",
    "L'exécution du script sur la deuxième URL a réussi :\n",
    "* Code de statut : **200 OK**.\n",
    "* L'extrait HTML retourné correspondait bien au début du code source de la page de l'article valide.\n",
    "\n",
    "**Conclusion de l'Étape :**\n",
    "Le téléchargement du code HTML brut via la bibliothèque `requests` fonctionne pour les URLs d'articles LinkedIn Pulse valides (au moins pour l'URL testée). Nous pouvons donc maintenant passer à l'étape suivante : utiliser `BeautifulSoup` pour analyser ce HTML et tenter d'en extraire le contenu texte, les likes et les commentaires. L'inspection manuelle de cette URL valide est nécessaire pour trouver les bons sélecteurs HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apres divers soucis niveau scraping linkedin nous testons une nouvelle methode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Identifiants LinkedIn chargés depuis .env (Utilisateur: gonzalez.nicolas@icloud.com)\n",
      "✅ Imports et configuration initiale terminés.\n"
     ]
    }
   ],
   "source": [
    "# --- Imports Nécessaires ---\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv # Pour charger le .env\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --- Charger les Variables d'Environnement (depuis le fichier .env) ---\n",
    "# Assurez-vous que le fichier .env est dans le dossier 01_collecte_donnees\n",
    "load_dotenv()\n",
    "LINKEDIN_USERNAME = os.getenv(\"LINKEDIN_USERNAME\")\n",
    "LINKEDIN_PASSWORD = os.getenv(\"LINKEDIN_PASSWORD\")\n",
    "\n",
    "# Vérifier si les identifiants ont été chargés\n",
    "if not LINKEDIN_USERNAME or not LINKEDIN_PASSWORD:\n",
    "    print(\"⚠️ERREUR: Identifiants LinkedIn non trouvés dans le fichier .env ou les variables d'environnement.\")\n",
    "    raise ValueError(\"Identifiants LinkedIn manquants.\")\n",
    "else:\n",
    "    print(f\"✅ Identifiants LinkedIn chargés depuis .env (Utilisateur: {LINKEDIN_USERNAME})\")\n",
    "\n",
    "print(\"✅ Imports et configuration initiale terminés.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fonctions Selenium (avec extract_article_content mise à jour) définies.\n"
     ]
    }
   ],
   "source": [
    "# --- Fonctions Selenium et Extraction (Mises à jour) ---\n",
    "\n",
    "def get_user_agent():\n",
    "    \"\"\"Retourne un User-Agent aléatoire\"\"\"\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0',\n",
    "    ]\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Configure et retourne un driver Selenium Chrome\"\"\"\n",
    "    print(\"\\nConfiguration du navigateur Chrome (Selenium)...\")\n",
    "    chrome_options = Options()\n",
    "    # Pour voir le navigateur, commentez la ligne suivante :\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(f\"user-agent={get_user_agent()}\")\n",
    "    chrome_options.add_argument(\"--disable-notifications\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--log-level=3\")\n",
    "\n",
    "    driver = None\n",
    "    try:\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        print(\"✅ Navigateur Chrome configuré.\")\n",
    "        return driver\n",
    "    except ValueError as ve:\n",
    "         print(f\"❌ Erreur de configuration WebDriver: {ve}\")\n",
    "         return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur inattendue config navigateur: {e}\")\n",
    "        if driver: driver.quit()\n",
    "        return None\n",
    "\n",
    "def linkedin_login(driver, username, password):\n",
    "    \"\"\"Se connecte à LinkedIn\"\"\"\n",
    "    if not driver: return False\n",
    "    print(\"\\nTentative de connexion à LinkedIn...\")\n",
    "    try:\n",
    "        login_url = \"https://www.linkedin.com/login?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin\"\n",
    "        driver.get(login_url)\n",
    "        print(f\" Accès à {login_url}\")\n",
    "        time.sleep(random.uniform(3, 5))\n",
    "\n",
    "        if \"feed\" in driver.current_url:\n",
    "            print(\" Déjà connecté.\")\n",
    "            return True\n",
    "\n",
    "        print(\" Remplissage du formulaire...\")\n",
    "        try:\n",
    "            username_field = WebDriverWait(driver, 15).until(EC.visibility_of_element_located((By.ID, \"username\")))\n",
    "            password_field = driver.find_element(By.ID, \"password\")\n",
    "            username_field.send_keys(username)\n",
    "            time.sleep(random.uniform(0.5, 1.2))\n",
    "            password_field.send_keys(password)\n",
    "            time.sleep(random.uniform(0.5, 1.2))\n",
    "            submit_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[data-litms-control-urn='login-submit'], button[type='submit']\")))\n",
    "            submit_button.click()\n",
    "            print(\" Bouton de connexion cliqué.\")\n",
    "            print(\" Attente après soumission (5-8 sec)...\")\n",
    "            time.sleep(random.uniform(5, 8))\n",
    "            current_url = driver.current_url\n",
    "            if \"feed\" in current_url:\n",
    "                print(\"✅ Connexion réussie.\")\n",
    "                return True\n",
    "            elif \"checkpoint\" in current_url:\n",
    "                print(\"⚠️ Connexion réussie mais checkpoint détecté.\")\n",
    "                return True # On essaie quand même\n",
    "            else:\n",
    "                print(f\"❌ Échec de la connexion. URL: {current_url}\")\n",
    "                try:\n",
    "                    error_element = driver.find_element(By.CSS_SELECTOR, \"[role='alert'], .form__input--error, #error-for-password, #error-for-username\")\n",
    "                    if error_element: print(f\" Message d'erreur: {error_element.text[:150]}\")\n",
    "                except NoSuchElementException: print(\" Aucun message d'erreur trouvé.\")\n",
    "                return False\n",
    "        except TimeoutException: print(\"❌ Timeout formulaire connexion.\"); return False\n",
    "        except Exception as e: print(f\"❌ Erreur formulaire : {e}\"); return False\n",
    "    except Exception as e: print(f\"❌ Erreur majeure connexion : {e}\"); return False\n",
    "\n",
    "def get_page_content_with_selenium(url, driver):\n",
    "    \"\"\"Récupère le HTML de la page cible avec Selenium\"\"\"\n",
    "    if not driver: return None\n",
    "    print(f\"\\nAccès à l'URL cible via Selenium : {url}\")\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        print(\" Attente chargement (5-8 sec)...\")\n",
    "        time.sleep(random.uniform(5, 8))\n",
    "        current_url = driver.current_url\n",
    "        if \"login\" in current_url or \"signup\" in current_url or \"authwall\" in current_url:\n",
    "             print(f\"⚠️ Redirection vers {current_url}.\")\n",
    "             return None\n",
    "        print(\" Défilement léger...\")\n",
    "        for i in range(2):\n",
    "             driver.execute_script(f\"window.scrollTo(0, {(i + 1) * 600});\")\n",
    "             print(f\" Scroll {i+1}/2...\")\n",
    "             time.sleep(random.uniform(1.5, 2.5))\n",
    "        try:\n",
    "             WebDriverWait(driver, 15).until(EC.visibility_of_element_located((By.TAG_NAME, \"h1\")))\n",
    "             print(\" Élément H1 visible.\")\n",
    "        except TimeoutException: print(\"⚠️ Timeout H1.\")\n",
    "        html_content = driver.page_source\n",
    "        print(\"✅ Contenu HTML récupéré.\")\n",
    "        return html_content\n",
    "    except Exception as e: print(f\"❌ Erreur récupération page : {e}\"); return None\n",
    "\n",
    "# --- NOUVELLE VERSION ICI ---\n",
    "def extract_article_content(html_content):\n",
    "    \"\"\"Extrait le contenu principal de l'article en utilisant le sélecteur .reader-article-content\"\"\"\n",
    "    if not html_content: return \"Contenu HTML non disponible\"\n",
    "    print(\"\\nAnalyse HTML pour contenu article (avec sélecteur .reader-article-content)...\")\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    article_text = \"\"\n",
    "    found_by = \"Aucune méthode\"\n",
    "    selector = 'div.reader-article-content' # <--- NOUVEAU SÉLECTEUR BASÉ SUR INSPECTION\n",
    "\n",
    "    try:\n",
    "        content_div = soup.select_one(selector)\n",
    "        if content_div:\n",
    "            content_parts = []\n",
    "            # Itérer sur les éléments pour extraire le texte\n",
    "            for element in content_div.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li'], recursive=True):\n",
    "                 text_part = element.get_text(strip=True)\n",
    "                 if text_part: content_parts.append(text_part)\n",
    "            if content_parts:\n",
    "                article_text = \"\\n\\n\".join(content_parts)\n",
    "                found_by = f\"Sélecteur '{selector}'\"\n",
    "                print(f\" Contenu trouvé via: {found_by}\")\n",
    "                return article_text.strip()\n",
    "            else: # Fallback texte brut\n",
    "                 article_text = content_div.get_text(separator='\\n', strip=True)\n",
    "                 if article_text:\n",
    "                      found_by = f\"Sélecteur '{selector}' (texte brut)\"\n",
    "                      print(f\" Contenu trouvé via: {found_by}\")\n",
    "                      return article_text.strip()\n",
    "    except Exception as e: print(f\" Erreur en cherchant contenu avec '{selector}': {e}\"); pass\n",
    "\n",
    "    if not article_text:\n",
    "        print(\"⚠️ Contenu article non trouvé avec nouveau sélecteur.\")\n",
    "        return \"Contenu non trouvé\"\n",
    "    return \"Contenu non trouvé (fin)\" # Ne devrait pas arriver\n",
    "# --- FIN NOUVELLE VERSION ---\n",
    "\n",
    "def extract_likes_count_with_selenium(driver):\n",
    "    \"\"\"Extrait le nombre de likes/réactions avec Selenium (méthode aria-label)\"\"\"\n",
    "    print(\"\\nRecherche des Likes/Réactions...\")\n",
    "    try:\n",
    "        xpath_selector = \"//button[contains(@aria-label, 'réaction') or contains(@aria-label, 'reaction')]\"\n",
    "        likes_element = WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.XPATH, xpath_selector)))\n",
    "        aria_label_text = likes_element.get_attribute(\"aria-label\")\n",
    "        match = re.search(r'^(\\d+)', aria_label_text)\n",
    "        if match:\n",
    "            likes_count = match.group(1)\n",
    "            print(f\" Likes trouvés via XPath aria-label: '{xpath_selector}' -> {likes_count}\")\n",
    "            return likes_count\n",
    "        else:\n",
    "             likes_text = likes_element.get_attribute(\"textContent\").strip()\n",
    "             match_text = re.search(r'^(\\d+)', likes_text)\n",
    "             if match_text:\n",
    "                  likes_count = match_text.group(1); print(f\" Likes trouvés via XPath (texte fallback): '{xpath_selector}' -> {likes_count}\"); return likes_count\n",
    "    except (NoSuchElementException, TimeoutException): print(\"⚠️ Élément Likes non trouvé via XPath.\")\n",
    "    except Exception as e: print(f\" Erreur mineure likes XPath: {e}\")\n",
    "    # Fallback CSS (au cas où)\n",
    "    try:\n",
    "        selector_css = \".social-details-social-counts__reactions-count\"; likes_element_css = WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.CSS_SELECTOR, selector_css))); likes_text_css = likes_element_css.get_attribute(\"textContent\").strip(); match_css = re.search(r'^(\\d+)', likes_text_css)\n",
    "        if match_css: likes_count_css = match_css.group(1); print(f\" Likes trouvés via CSS fallback: '{selector_css}' -> {likes_count_css}\"); return likes_count_css\n",
    "    except Exception: print(\"⚠️ Likes non trouvés via CSS non plus.\")\n",
    "    return \"Non trouvé\"\n",
    "\n",
    "def extract_comments_count_with_selenium(driver):\n",
    "    \"\"\"Extrait le nombre de commentaires avec Selenium (méthode aria-label)\"\"\"\n",
    "    print(\"\\nRecherche des Commentaires...\")\n",
    "    try:\n",
    "        xpath_selector = \"//button[contains(@aria-label, 'commentaire') or contains(@aria-label, 'comment')]\"\n",
    "        # Alternative : xpath_selector = \"//a[contains(@aria-label, 'commentaire') or contains(@aria-label, 'comment')]\"\n",
    "        comments_element = WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.XPATH, xpath_selector)))\n",
    "        aria_label_text = comments_element.get_attribute(\"aria-label\")\n",
    "        match = re.search(r'^(\\d+)', aria_label_text)\n",
    "        if match:\n",
    "            comments_count = match.group(1); print(f\" Commentaires trouvés via XPath aria-label: '{xpath_selector}' -> {comments_count}\"); return comments_count\n",
    "        else:\n",
    "             comments_text = comments_element.get_attribute(\"textContent\").strip(); match_text = re.search(r'^(\\d+)', comments_text)\n",
    "             if match_text: comments_count = match_text.group(1); print(f\" Commentaires trouvés via XPath (texte fallback): '{xpath_selector}' -> {comments_count}\"); return comments_count\n",
    "    except (NoSuchElementException, TimeoutException): print(\"⚠️ Élément Commentaires non trouvé via XPath.\")\n",
    "    except Exception as e: print(f\" Erreur mineure commentaires XPath: {e}\")\n",
    "    # Ajouter fallbacks CSS si nécessaire\n",
    "    print(\"⚠️ Nombre de commentaires non trouvé via sélecteurs principaux.\")\n",
    "    return \"Non trouvé\"\n",
    "\n",
    "print(\"✅ Fonctions Selenium (avec extract_article_content mise à jour) définies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2 : Succès de l'Extraction Initiale !\n",
    "\n",
    "**Bilan de l'Exécution Précédente :**\n",
    "Le script Selenium, exécuté sur la **deuxième URL** de test (article sur l'IA générative), a réussi à :\n",
    "1.  Se connecter à LinkedIn avec les identifiants du fichier `.env`.\n",
    "2.  Accéder à la page de l'article après la connexion.\n",
    "3.  Récupérer le code HTML complet de la page.\n",
    "4.  Utiliser les fonctions d'extraction (avec les sélecteurs affinés) pour obtenir :\n",
    "    * ✅ Le **contenu** principal de l'article (via `div.reader-article-content`).\n",
    "    * ✅ Le nombre de **likes/réactions** (36, via XPath/aria-label).\n",
    "    * ✅ Le nombre de **commentaires** (16, via XPath/aria-label).\n",
    "\n",
    "**Conclusion :** Le processus d'extraction pour une URL unique est maintenant fonctionnel !\n",
    "\n",
    "---\n",
    "\n",
    "**Prochaine Étape : Structurer les Données Extraites**\n",
    "\n",
    "Avant de modifier le script pour traiter *toutes* les URLs de la liste, nous allons d'abord améliorer la façon dont les résultats sont gérés pour une seule URL.\n",
    "\n",
    "**Action immédiate :** Modifier la fin du code de la cellule d'exécution précédente pour que les informations extraites (URL, contenu, likes, commentaires) soient stockées dans un **dictionnaire Python** au lieu d'être simplement affichées séparément. Cela facilitera ensuite la collecte des résultats pour plusieurs URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Identifiants LinkedIn chargés depuis .env (Utilisateur: gonzalez.nicolas@icloud.com)\n",
      "✅ Imports et configuration initiale terminés.\n"
     ]
    }
   ],
   "source": [
    "# --- Imports Nécessaires ---\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv # Pour charger le .env\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --- Charger les Variables d'Environnement (depuis le fichier .env) ---\n",
    "# Assurez-vous que le fichier .env est dans le dossier 01_collecte_donnees\n",
    "load_dotenv()\n",
    "LINKEDIN_USERNAME = os.getenv(\"LINKEDIN_USERNAME\")\n",
    "LINKEDIN_PASSWORD = os.getenv(\"LINKEDIN_PASSWORD\")\n",
    "\n",
    "# Vérifier si les identifiants ont été chargés\n",
    "if not LINKEDIN_USERNAME or not LINKEDIN_PASSWORD:\n",
    "    print(\"⚠️ERREUR: Identifiants LinkedIn non trouvés dans le fichier .env ou les variables d'environnement.\")\n",
    "    raise ValueError(\"Identifiants LinkedIn manquants.\")\n",
    "else:\n",
    "    print(f\"✅ Identifiants LinkedIn chargés depuis .env (Utilisateur: {LINKEDIN_USERNAME})\")\n",
    "\n",
    "print(\"✅ Imports et configuration initiale terminés.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fonctions Selenium (avec extract_article_content mise à jour) définies.\n"
     ]
    }
   ],
   "source": [
    "# --- Fonctions Selenium et Extraction (Mises à jour) ---\n",
    "\n",
    "def get_user_agent():\n",
    "    \"\"\"Retourne un User-Agent aléatoire\"\"\"\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0',\n",
    "    ]\n",
    "    return random.choice(user_agents)\n",
    "\n",
    "def setup_driver():\n",
    "    \"\"\"Configure et retourne un driver Selenium Chrome\"\"\"\n",
    "    print(\"\\nConfiguration du navigateur Chrome (Selenium)...\")\n",
    "    chrome_options = Options()\n",
    "    # Pour voir le navigateur, commentez la ligne suivante :\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    chrome_options.add_argument(f\"user-agent={get_user_agent()}\")\n",
    "    chrome_options.add_argument(\"--disable-notifications\")\n",
    "    chrome_options.add_argument(\"--disable-extensions\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "    chrome_options.add_argument(\"--log-level=3\")\n",
    "\n",
    "    driver = None\n",
    "    try:\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "        print(\"✅ Navigateur Chrome configuré.\")\n",
    "        return driver\n",
    "    except ValueError as ve:\n",
    "         print(f\"❌ Erreur de configuration WebDriver: {ve}\")\n",
    "         return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur inattendue config navigateur: {e}\")\n",
    "        if driver: driver.quit()\n",
    "        return None\n",
    "\n",
    "def linkedin_login(driver, username, password):\n",
    "    \"\"\"Se connecte à LinkedIn\"\"\"\n",
    "    if not driver: return False\n",
    "    print(\"\\nTentative de connexion à LinkedIn...\")\n",
    "    try:\n",
    "        login_url = \"https://www.linkedin.com/login?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin\"\n",
    "        driver.get(login_url)\n",
    "        print(f\" Accès à {login_url}\")\n",
    "        time.sleep(random.uniform(3, 5))\n",
    "\n",
    "        if \"feed\" in driver.current_url:\n",
    "            print(\" Déjà connecté.\")\n",
    "            return True\n",
    "\n",
    "        print(\" Remplissage du formulaire...\")\n",
    "        try:\n",
    "            username_field = WebDriverWait(driver, 15).until(EC.visibility_of_element_located((By.ID, \"username\")))\n",
    "            password_field = driver.find_element(By.ID, \"password\")\n",
    "            username_field.send_keys(username)\n",
    "            time.sleep(random.uniform(0.5, 1.2))\n",
    "            password_field.send_keys(password)\n",
    "            time.sleep(random.uniform(0.5, 1.2))\n",
    "            submit_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, \"button[data-litms-control-urn='login-submit'], button[type='submit']\")))\n",
    "            submit_button.click()\n",
    "            print(\" Bouton de connexion cliqué.\")\n",
    "            print(\" Attente après soumission (5-8 sec)...\")\n",
    "            time.sleep(random.uniform(5, 8))\n",
    "            current_url = driver.current_url\n",
    "            if \"feed\" in current_url:\n",
    "                print(\"✅ Connexion réussie.\")\n",
    "                return True\n",
    "            elif \"checkpoint\" in current_url:\n",
    "                print(\"⚠️ Connexion réussie mais checkpoint détecté.\")\n",
    "                return True # On essaie quand même\n",
    "            else:\n",
    "                print(f\"❌ Échec de la connexion. URL: {current_url}\")\n",
    "                try:\n",
    "                    error_element = driver.find_element(By.CSS_SELECTOR, \"[role='alert'], .form__input--error, #error-for-password, #error-for-username\")\n",
    "                    if error_element: print(f\" Message d'erreur: {error_element.text[:150]}\")\n",
    "                except NoSuchElementException: print(\" Aucun message d'erreur trouvé.\")\n",
    "                return False\n",
    "        except TimeoutException: print(\"❌ Timeout formulaire connexion.\"); return False\n",
    "        except Exception as e: print(f\"❌ Erreur formulaire : {e}\"); return False\n",
    "    except Exception as e: print(f\"❌ Erreur majeure connexion : {e}\"); return False\n",
    "\n",
    "def get_page_content_with_selenium(url, driver):\n",
    "    \"\"\"Récupère le HTML de la page cible avec Selenium\"\"\"\n",
    "    if not driver: return None\n",
    "    print(f\"\\nAccès à l'URL cible via Selenium : {url}\")\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        print(\" Attente chargement (5-8 sec)...\")\n",
    "        time.sleep(random.uniform(5, 8))\n",
    "        current_url = driver.current_url\n",
    "        if \"login\" in current_url or \"signup\" in current_url or \"authwall\" in current_url:\n",
    "             print(f\"⚠️ Redirection vers {current_url}.\")\n",
    "             return None\n",
    "        print(\" Défilement léger...\")\n",
    "        for i in range(2):\n",
    "             driver.execute_script(f\"window.scrollTo(0, {(i + 1) * 600});\")\n",
    "             print(f\" Scroll {i+1}/2...\")\n",
    "             time.sleep(random.uniform(1.5, 2.5))\n",
    "        try:\n",
    "             WebDriverWait(driver, 15).until(EC.visibility_of_element_located((By.TAG_NAME, \"h1\")))\n",
    "             print(\" Élément H1 visible.\")\n",
    "        except TimeoutException: print(\"⚠️ Timeout H1.\")\n",
    "        html_content = driver.page_source\n",
    "        print(\"✅ Contenu HTML récupéré.\")\n",
    "        return html_content\n",
    "    except Exception as e: print(f\"❌ Erreur récupération page : {e}\"); return None\n",
    "\n",
    "# --- NOUVELLE VERSION ICI ---\n",
    "def extract_article_content(html_content):\n",
    "    \"\"\"Extrait le contenu principal de l'article en utilisant le sélecteur .reader-article-content\"\"\"\n",
    "    if not html_content: return \"Contenu HTML non disponible\"\n",
    "    print(\"\\nAnalyse HTML pour contenu article (avec sélecteur .reader-article-content)...\")\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    article_text = \"\"\n",
    "    found_by = \"Aucune méthode\"\n",
    "    selector = 'div.reader-article-content' # <--- NOUVEAU SÉLECTEUR BASÉ SUR INSPECTION\n",
    "\n",
    "    try:\n",
    "        content_div = soup.select_one(selector)\n",
    "        if content_div:\n",
    "            content_parts = []\n",
    "            # Itérer sur les éléments pour extraire le texte\n",
    "            for element in content_div.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li'], recursive=True):\n",
    "                 text_part = element.get_text(strip=True)\n",
    "                 if text_part: content_parts.append(text_part)\n",
    "            if content_parts:\n",
    "                article_text = \"\\n\\n\".join(content_parts)\n",
    "                found_by = f\"Sélecteur '{selector}'\"\n",
    "                print(f\" Contenu trouvé via: {found_by}\")\n",
    "                return article_text.strip()\n",
    "            else: # Fallback texte brut\n",
    "                 article_text = content_div.get_text(separator='\\n', strip=True)\n",
    "                 if article_text:\n",
    "                      found_by = f\"Sélecteur '{selector}' (texte brut)\"\n",
    "                      print(f\" Contenu trouvé via: {found_by}\")\n",
    "                      return article_text.strip()\n",
    "    except Exception as e: print(f\" Erreur en cherchant contenu avec '{selector}': {e}\"); pass\n",
    "\n",
    "    if not article_text:\n",
    "        print(\"⚠️ Contenu article non trouvé avec nouveau sélecteur.\")\n",
    "        return \"Contenu non trouvé\"\n",
    "    return \"Contenu non trouvé (fin)\" # Ne devrait pas arriver\n",
    "# --- FIN NOUVELLE VERSION ---\n",
    "\n",
    "def extract_likes_count_with_selenium(driver):\n",
    "    \"\"\"Extrait le nombre de likes/réactions avec Selenium (méthode aria-label)\"\"\"\n",
    "    print(\"\\nRecherche des Likes/Réactions...\")\n",
    "    try:\n",
    "        xpath_selector = \"//button[contains(@aria-label, 'réaction') or contains(@aria-label, 'reaction')]\"\n",
    "        likes_element = WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.XPATH, xpath_selector)))\n",
    "        aria_label_text = likes_element.get_attribute(\"aria-label\")\n",
    "        match = re.search(r'^(\\d+)', aria_label_text)\n",
    "        if match:\n",
    "            likes_count = match.group(1)\n",
    "            print(f\" Likes trouvés via XPath aria-label: '{xpath_selector}' -> {likes_count}\")\n",
    "            return likes_count\n",
    "        else:\n",
    "             likes_text = likes_element.get_attribute(\"textContent\").strip()\n",
    "             match_text = re.search(r'^(\\d+)', likes_text)\n",
    "             if match_text:\n",
    "                  likes_count = match_text.group(1); print(f\" Likes trouvés via XPath (texte fallback): '{xpath_selector}' -> {likes_count}\"); return likes_count\n",
    "    except (NoSuchElementException, TimeoutException): print(\"⚠️ Élément Likes non trouvé via XPath.\")\n",
    "    except Exception as e: print(f\" Erreur mineure likes XPath: {e}\")\n",
    "    # Fallback CSS (au cas où)\n",
    "    try:\n",
    "        selector_css = \".social-details-social-counts__reactions-count\"; likes_element_css = WebDriverWait(driver, 5).until(EC.visibility_of_element_located((By.CSS_SELECTOR, selector_css))); likes_text_css = likes_element_css.get_attribute(\"textContent\").strip(); match_css = re.search(r'^(\\d+)', likes_text_css)\n",
    "        if match_css: likes_count_css = match_css.group(1); print(f\" Likes trouvés via CSS fallback: '{selector_css}' -> {likes_count_css}\"); return likes_count_css\n",
    "    except Exception: print(\"⚠️ Likes non trouvés via CSS non plus.\")\n",
    "    return \"Non trouvé\"\n",
    "\n",
    "def extract_comments_count_with_selenium(driver):\n",
    "    \"\"\"Extrait le nombre de commentaires avec Selenium (méthode aria-label)\"\"\"\n",
    "    print(\"\\nRecherche des Commentaires...\")\n",
    "    try:\n",
    "        xpath_selector = \"//button[contains(@aria-label, 'commentaire') or contains(@aria-label, 'comment')]\"\n",
    "        # Alternative : xpath_selector = \"//a[contains(@aria-label, 'commentaire') or contains(@aria-label, 'comment')]\"\n",
    "        comments_element = WebDriverWait(driver, 10).until(EC.visibility_of_element_located((By.XPATH, xpath_selector)))\n",
    "        aria_label_text = comments_element.get_attribute(\"aria-label\")\n",
    "        match = re.search(r'^(\\d+)', aria_label_text)\n",
    "        if match:\n",
    "            comments_count = match.group(1); print(f\" Commentaires trouvés via XPath aria-label: '{xpath_selector}' -> {comments_count}\"); return comments_count\n",
    "        else:\n",
    "             comments_text = comments_element.get_attribute(\"textContent\").strip(); match_text = re.search(r'^(\\d+)', comments_text)\n",
    "             if match_text: comments_count = match_text.group(1); print(f\" Commentaires trouvés via XPath (texte fallback): '{xpath_selector}' -> {comments_count}\"); return comments_count\n",
    "    except (NoSuchElementException, TimeoutException): print(\"⚠️ Élément Commentaires non trouvé via XPath.\")\n",
    "    except Exception as e: print(f\" Erreur mineure commentaires XPath: {e}\")\n",
    "    # Ajouter fallbacks CSS si nécessaire\n",
    "    print(\"⚠️ Nombre de commentaires non trouvé via sélecteurs principaux.\")\n",
    "    return \"Non trouvé\"\n",
    "\n",
    "print(\"✅ Fonctions Selenium (avec extract_article_content mise à jour) définies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cellule 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL Cible pour ce test : https://fr.linkedin.com/pulse/derni%C3%A8res-avanc%C3%A9es-en-ia-g%C3%A9n%C3%A9rative-mars-2025-arnault-chatel-zs39e\n",
      "\n",
      "Configuration du navigateur Chrome (Selenium)...\n",
      "✅ Navigateur Chrome configuré.\n",
      "\n",
      "Tentative de connexion à LinkedIn...\n",
      " Accès à https://www.linkedin.com/login?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin\n",
      " Remplissage du formulaire...\n",
      " Bouton de connexion cliqué.\n",
      " Attente après soumission (5-8 sec)...\n",
      "✅ Connexion réussie.\n",
      "\n",
      "Accès à l'URL cible via Selenium : https://fr.linkedin.com/pulse/derni%C3%A8res-avanc%C3%A9es-en-ia-g%C3%A9n%C3%A9rative-mars-2025-arnault-chatel-zs39e\n",
      " Attente chargement (5-8 sec)...\n",
      " Défilement léger...\n",
      " Scroll 1/2...\n",
      " Scroll 2/2...\n",
      " Élément H1 visible.\n",
      "✅ Contenu HTML récupéré.\n",
      "\n",
      "Analyse HTML pour contenu article (avec sélecteur .reader-article-content)...\n",
      " Contenu trouvé via: Sélecteur 'div.reader-article-content'\n",
      "\n",
      "Recherche des Likes/Réactions...\n",
      " Likes trouvés via XPath aria-label: '//button[contains(@aria-label, 'réaction') or contains(@aria-label, 'reaction')]' -> 16\n",
      "\n",
      "Recherche des Commentaires...\n",
      " Commentaires trouvés via XPath aria-label: '//button[contains(@aria-label, 'commentaire') or contains(@aria-label, 'comment')]' -> 3\n",
      "\n",
      "✅ Navigateur Selenium fermé.\n",
      "\n",
      "==================================================\n",
      "RÉSULTAT STRUCTURÉ POUR L'URL TESTÉE (Dictionnaire)\n",
      "==================================================\n",
      "- URL: https://fr.linkedin.com/pulse/derni%C3%A8res-avanc%C3%A9es-en-ia-g%C3%A9n%C3%A9rative-mars-2025-arnault-chatel-zs39e\n",
      "- Likes: 16\n",
      "- Commentaires: 3\n",
      "- Contenu (Extrait): La vitesse d’innovation en intelligence artificielle générative ne faiblit pas en ce début 2025. Des modèles deLarge Language Models(LLM) toujours plus puissants voient le jour, qu’ils soient le fruit...\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Logique Principale d'Exécution ---\n",
    "\n",
    "# Imports nécessaires au cas où la cellule est exécutée seule (normalement faits en Cellule 1)\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "# Assurez-vous que les variables LINKEDIN_USERNAME et LINKEDIN_PASSWORD sont chargées (normalement fait en Cellule 1)\n",
    "# Assurez-vous que les fonctions (setup_driver, linkedin_login, get_page_content_with_selenium, etc.) sont définies (normalement fait en Cellule 2)\n",
    "\n",
    "urls_filename = \"linkedin_urls_found.txt\" # Fichier créé par Phase 1\n",
    "target_url = None\n",
    "driver = None # Important d'initialiser à None\n",
    "\n",
    "# --- Charger l'URL cible (la deuxième) ---\n",
    "linkedin_urls = []\n",
    "# Vérifier si le fichier existe dans le dossier courant (normalement 01_collecte_donnees)\n",
    "if os.path.exists(urls_filename):\n",
    "    try:\n",
    "        with open(urls_filename, \"r\") as f:\n",
    "            linkedin_urls = [line.strip() for line in f if line.strip()]\n",
    "        if len(linkedin_urls) >= 2:\n",
    "            target_url = linkedin_urls[1] # CIBLER LA DEUXIÈME URL\n",
    "            print(f\"URL Cible pour ce test : {target_url}\")\n",
    "        elif linkedin_urls:\n",
    "             print(f\"⚠️ Fichier '{urls_filename}' contient seulement {len(linkedin_urls)} URL(s). Impossible de tester la deuxième.\")\n",
    "             target_url = None\n",
    "        else:\n",
    "            print(f\"⚠️ Le fichier '{urls_filename}' est vide.\")\n",
    "            target_url = None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors de la lecture du fichier URLs : {e}\")\n",
    "        target_url = None\n",
    "else:\n",
    "    print(f\"❌ Fichier URLs '{urls_filename}' non trouvé.\")\n",
    "    target_url = None\n",
    "\n",
    "# --- Exécuter seulement si on a une URL cible et les identifiants ---\n",
    "# Vérifier aussi que les fonctions existent (au cas où Cellule 2 n'a pas été exécutée)\n",
    "if target_url and 'LINKEDIN_USERNAME' in locals() and LINKEDIN_USERNAME and 'LINKEDIN_PASSWORD' in locals() and LINKEDIN_PASSWORD and 'setup_driver' in globals():\n",
    "    article_content = \"Non extrait\"\n",
    "    likes_count = \"Non extrait\"\n",
    "    comments_count = \"Non extrait\"\n",
    "    resultat_url = {} # Initialiser le dictionnaire de résultat\n",
    "\n",
    "    try:\n",
    "        # 1. Démarrer le navigateur\n",
    "        driver = setup_driver() # Défini dans la cellule précédente\n",
    "\n",
    "        if driver:\n",
    "            # 2. Se connecter\n",
    "            login_success = linkedin_login(driver, LINKEDIN_USERNAME, LINKEDIN_PASSWORD) # Défini dans la cellule précédente\n",
    "\n",
    "            if login_success:\n",
    "                # 3. Récupérer le HTML de la page cible\n",
    "                html_content = get_page_content_with_selenium(target_url, driver) # Défini dans la cellule précédente\n",
    "\n",
    "                if html_content:\n",
    "                    # 4. Extraire le contenu texte\n",
    "                    # Utilise la fonction MISE À JOUR définie précédemment\n",
    "                    article_content = extract_article_content(html_content)\n",
    "                    # 5. Extraire Likes et Commentaires (via Selenium sur la page chargée)\n",
    "                    likes_count = extract_likes_count_with_selenium(driver) # Défini dans la cellule précédente\n",
    "                    comments_count = extract_comments_count_with_selenium(driver) # Défini dans la cellule précédente\n",
    "                else:\n",
    "                    article_content = \"Erreur: Impossible de récupérer le HTML de la page cible.\"\n",
    "                    # Assigner aussi les erreurs aux autres comptes si HTML échoue\n",
    "                    likes_count = \"Erreur: HTML non récupéré\"\n",
    "                    comments_count = \"Erreur: HTML non récupéré\"\n",
    "            else:\n",
    "                article_content = \"Erreur: Échec de la connexion LinkedIn.\"\n",
    "                likes_count = \"Erreur: Connexion échouée\"\n",
    "                comments_count = \"Erreur: Connexion échouée\"\n",
    "        else:\n",
    "             article_content = \"Erreur: Driver Selenium non initialisé.\"\n",
    "             likes_count = \"Erreur: Driver non initialisé\"\n",
    "             comments_count = \"Erreur: Driver non initialisé\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Une erreur majeure est survenue lors de l'exécution principale : {e}\")\n",
    "        article_content = f\"Erreur: {e}\"\n",
    "        # Assurer que les autres variables ont aussi une valeur d'erreur\n",
    "        if 'likes_count' not in locals() or likes_count == \"Non extrait\": likes_count = f\"Erreur: {e}\"\n",
    "        if 'comments_count' not in locals() or comments_count == \"Non extrait\": comments_count = f\"Erreur: {e}\"\n",
    "    finally:\n",
    "        # 6. Fermer le navigateur proprement, quoi qu'il arrive\n",
    "        if driver:\n",
    "            try:\n",
    "                driver.quit()\n",
    "                print(\"\\n✅ Navigateur Selenium fermé.\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Erreur lors de la fermeture du navigateur: {e}\")\n",
    "\n",
    "    # --- Stocker et Afficher les résultats finaux pour cette URL ---\n",
    "    # Utilisation de la logique modifiée pour créer et afficher le dictionnaire\n",
    "    resultat_url = {\n",
    "        \"url\": target_url if target_url else \"URL non définie\",\n",
    "        \"likes\": likes_count, # Contient déjà la valeur ou un message d'erreur\n",
    "        \"commentaires\": comments_count, # Contient déjà la valeur ou un message d'erreur\n",
    "        \"contenu\": article_content if isinstance(article_content, str) else f\"Erreur: {article_content}\" # Stocker l'erreur si contenu non str\n",
    "    }\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RÉSULTAT STRUCTURÉ POUR L'URL TESTÉE (Dictionnaire)\")\n",
    "    print(\"=\"*50)\n",
    "    # Afficher les clés et valeurs du dictionnaire pour vérification\n",
    "    print(f\"- URL: {resultat_url.get('url', 'N/A')}\")\n",
    "    print(f\"- Likes: {resultat_url.get('likes', 'N/A')}\")\n",
    "    print(f\"- Commentaires: {resultat_url.get('commentaires', 'N/A')}\")\n",
    "    contenu_extrait = resultat_url.get('contenu', '')\n",
    "    # Gérer le cas où le contenu est une erreur et n'a pas de len()\n",
    "    if isinstance(contenu_extrait, str):\n",
    "        print(f\"- Contenu (Extrait): {contenu_extrait[:200] + '...' if len(contenu_extrait) > 200 else contenu_extrait}\")\n",
    "    else:\n",
    "        print(f\"- Contenu (Erreur): {contenu_extrait}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # Note : Dans l'étape suivante (boucle), on ajoutera ce dictionnaire 'resultat_url'\n",
    "    # à une liste globale : all_results.append(resultat_url)\n",
    "\n",
    "# Gérer les cas où l'exécution n'a pas eu lieu car target_url était None ou identifiants manquants\n",
    "elif not target_url:\n",
    "    print(\"\\nAucune URL cible à traiter (vérifiez le fichier d'URLs ou le code de chargement).\")\n",
    "elif not ('LINKEDIN_USERNAME' in locals() and LINKEDIN_USERNAME and 'LINKEDIN_PASSWORD' in locals() and LINKEDIN_PASSWORD):\n",
    "     print(\"\\nIdentifiants LinkedIn manquants, impossible de continuer (vérifiez l'exécution de la Cellule 1 et le fichier .env).\")\n",
    "else:\n",
    "     print(\"\\nProblème inconnu avant le démarrage de Selenium.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maintnenat on lance le grand test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 10 URLs chargées depuis 'linkedin_urls_found.txt' pour traitement.\n",
      "\n",
      "--- Démarrage du processus de scraping en boucle ---\n",
      "\n",
      "Configuration du navigateur Chrome (Selenium)...\n",
      "✅ Navigateur Chrome configuré.\n",
      "\n",
      "Tentative de connexion à LinkedIn...\n",
      " Accès à https://www.linkedin.com/login?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin\n",
      " Remplissage du formulaire...\n",
      " Bouton de connexion cliqué.\n",
      " Attente après soumission (5-8 sec)...\n",
      "✅ Connexion réussie.\n",
      "\n",
      "------------------------------\n",
      "Traitement URL 1/10 : https://fr.linkedin.com/pulse/comment-arbitrer-entre-ia-classique-et-g%C3%A9n%C3%A9rative-le-de-mascureau-2drce\n",
      "------------------------------\n",
      "\n",
      "Accès à l'URL cible via Selenium : https://fr.linkedin.com/pulse/comment-arbitrer-entre-ia-classique-et-g%C3%A9n%C3%A9rative-le-de-mascureau-2drce\n",
      " Attente chargement (5-8 sec)...\n",
      " Défilement léger...\n",
      " Scroll 1/2...\n",
      " Scroll 2/2...\n",
      " Élément H1 visible.\n",
      "✅ Contenu HTML récupéré.\n",
      "\n",
      "Analyse HTML pour contenu article (avec sélecteur .reader-article-content)...\n",
      " Contenu trouvé via: Sélecteur 'div.reader-article-content'\n",
      "\n",
      "Recherche des Likes/Réactions...\n",
      " Likes trouvés via XPath aria-label: '//button[contains(@aria-label, 'réaction') or contains(@aria-label, 'reaction')]' -> 78\n",
      "\n",
      "Recherche des Commentaires...\n",
      " Commentaires trouvés via XPath aria-label: '//button[contains(@aria-label, 'commentaire') or contains(@aria-label, 'comment')]' -> 4\n",
      "-> Résultat ajouté pour https://fr.linkedin.com/pulse/comment-arbitrer-entre-ia-classique-et-g%C3%A9n%C3%A9rative-le-de-mascureau-2drce\n",
      "   Likes: 78, Commentaires: 4\n",
      "--- Pause de 18.4 secondes avant la prochaine URL ---\n",
      "\n",
      "------------------------------\n",
      "Traitement URL 2/10 : https://fr.linkedin.com/pulse/derni%C3%A8res-avanc%C3%A9es-en-ia-g%C3%A9n%C3%A9rative-mars-2025-arnault-chatel-zs39e\n",
      "------------------------------\n",
      "\n",
      "Accès à l'URL cible via Selenium : https://fr.linkedin.com/pulse/derni%C3%A8res-avanc%C3%A9es-en-ia-g%C3%A9n%C3%A9rative-mars-2025-arnault-chatel-zs39e\n",
      " Attente chargement (5-8 sec)...\n",
      " Défilement léger...\n",
      " Scroll 1/2...\n",
      " Scroll 2/2...\n",
      " Élément H1 visible.\n",
      "✅ Contenu HTML récupéré.\n",
      "\n",
      "Analyse HTML pour contenu article (avec sélecteur .reader-article-content)...\n",
      " Contenu trouvé via: Sélecteur 'div.reader-article-content'\n",
      "\n",
      "Recherche des Likes/Réactions...\n",
      " Likes trouvés via XPath aria-label: '//button[contains(@aria-label, 'réaction') or contains(@aria-label, 'reaction')]' -> 16\n",
      "\n",
      "Recherche des Commentaires...\n",
      " Commentaires trouvés via XPath aria-label: '//button[contains(@aria-label, 'commentaire') or contains(@aria-label, 'comment')]' -> 3\n",
      "-> Résultat ajouté pour https://fr.linkedin.com/pulse/derni%C3%A8res-avanc%C3%A9es-en-ia-g%C3%A9n%C3%A9rative-mars-2025-arnault-chatel-zs39e\n",
      "   Likes: 16, Commentaires: 3\n",
      "--- Pause de 13.7 secondes avant la prochaine URL ---\n",
      "\n",
      "------------------------------\n",
      "Traitement URL 3/10 : https://fr.linkedin.com/pulse/ia-g%C3%A9n%C3%A9rative-les-d%C3%A9fis-r%C3%A9glementaires-%C3%A9thiques-et-pour-manchau-phdge\n",
      "------------------------------\n",
      "\n",
      "Accès à l'URL cible via Selenium : https://fr.linkedin.com/pulse/ia-g%C3%A9n%C3%A9rative-les-d%C3%A9fis-r%C3%A9glementaires-%C3%A9thiques-et-pour-manchau-phdge\n",
      " Attente chargement (5-8 sec)...\n",
      " Défilement léger...\n",
      " Scroll 1/2...\n",
      " Scroll 2/2...\n",
      " Élément H1 visible.\n",
      "✅ Contenu HTML récupéré.\n",
      "\n",
      "Analyse HTML pour contenu article (avec sélecteur .reader-article-content)...\n",
      " Contenu trouvé via: Sélecteur 'div.reader-article-content'\n",
      "\n",
      "Recherche des Likes/Réactions...\n",
      " Likes trouvés via XPath aria-label: '//button[contains(@aria-label, 'réaction') or contains(@aria-label, 'reaction')]' -> 1\n",
      "\n",
      "Recherche des Commentaires...\n",
      " Commentaires trouvés via XPath aria-label: '//button[contains(@aria-label, 'commentaire') or contains(@aria-label, 'comment')]' -> 2\n",
      "-> Résultat ajouté pour https://fr.linkedin.com/pulse/ia-g%C3%A9n%C3%A9rative-les-d%C3%A9fis-r%C3%A9glementaires-%C3%A9thiques-et-pour-manchau-phdge\n",
      "   Likes: 1, Commentaires: 2\n",
      "--- Pause de 17.7 secondes avant la prochaine URL ---\n",
      "\n",
      "------------------------------\n",
      "Traitement URL 4/10 : https://fr.linkedin.com/pulse/intelligence-artificielle-g%C3%A9n%C3%A9rative-r%C3%A9volution-et-applications-urdhe\n",
      "------------------------------\n",
      "\n",
      "Accès à l'URL cible via Selenium : https://fr.linkedin.com/pulse/intelligence-artificielle-g%C3%A9n%C3%A9rative-r%C3%A9volution-et-applications-urdhe\n",
      " Attente chargement (5-8 sec)...\n",
      " Défilement léger...\n",
      " Scroll 1/2...\n",
      " Scroll 2/2...\n",
      " Élément H1 visible.\n",
      "✅ Contenu HTML récupéré.\n",
      "\n",
      "Analyse HTML pour contenu article (avec sélecteur .reader-article-content)...\n",
      " Contenu trouvé via: Sélecteur 'div.reader-article-content'\n",
      "\n",
      "Recherche des Likes/Réactions...\n",
      " Likes trouvés via XPath aria-label: '//button[contains(@aria-label, 'réaction') or contains(@aria-label, 'reaction')]' -> 9\n",
      "\n",
      "Recherche des Commentaires...\n",
      " Commentaires trouvés via XPath aria-label: '//button[contains(@aria-label, 'commentaire') or contains(@aria-label, 'comment')]' -> 3\n",
      "-> Résultat ajouté pour https://fr.linkedin.com/pulse/intelligence-artificielle-g%C3%A9n%C3%A9rative-r%C3%A9volution-et-applications-urdhe\n",
      "   Likes: 9, Commentaires: 3\n",
      "--- Pause de 15.3 secondes avant la prochaine URL ---\n",
      "\n",
      "------------------------------\n",
      "Traitement URL 5/10 : https://fr.linkedin.com/pulse/intelligence-artificielle-ia-g%C3%A9n%C3%A9rative-%C3%A0-la-chatgpt-et-bughin\n",
      "------------------------------\n",
      "\n",
      "Accès à l'URL cible via Selenium : https://fr.linkedin.com/pulse/intelligence-artificielle-ia-g%C3%A9n%C3%A9rative-%C3%A0-la-chatgpt-et-bughin\n",
      " Attente chargement (5-8 sec)...\n",
      " Défilement léger...\n",
      " Scroll 1/2...\n",
      " Scroll 2/2...\n",
      " Élément H1 visible.\n",
      "✅ Contenu HTML récupéré.\n",
      "\n",
      "Analyse HTML pour contenu article (avec sélecteur .reader-article-content)...\n",
      " Contenu trouvé via: Sélecteur 'div.reader-article-content'\n",
      "\n",
      "Recherche des Likes/Réactions...\n",
      " Likes trouvés via XPath aria-label: '//button[contains(@aria-label, 'réaction') or contains(@aria-label, 'reaction')]' -> 7\n",
      "\n",
      "Recherche des Commentaires...\n",
      " Commentaires trouvés via XPath aria-label: '//button[contains(@aria-label, 'commentaire') or contains(@aria-label, 'comment')]' -> 2\n",
      "-> Résultat ajouté pour https://fr.linkedin.com/pulse/intelligence-artificielle-ia-g%C3%A9n%C3%A9rative-%C3%A0-la-chatgpt-et-bughin\n",
      "   Likes: 7, Commentaires: 2\n",
      "--- Pause de 12.5 secondes avant la prochaine URL ---\n",
      "\n",
      "------------------------------\n",
      "Traitement URL 6/10 : https://fr.linkedin.com/pulse/lai-act-encadrer-lintelligence-artificielle-pour-un-futur-spitz-xks0e\n",
      "------------------------------\n",
      "\n",
      "Accès à l'URL cible via Selenium : https://fr.linkedin.com/pulse/lai-act-encadrer-lintelligence-artificielle-pour-un-futur-spitz-xks0e\n",
      " Attente chargement (5-8 sec)...\n",
      " Défilement léger...\n",
      " Scroll 1/2...\n",
      " Scroll 2/2...\n",
      " Élément H1 visible.\n",
      "✅ Contenu HTML récupéré.\n",
      "\n",
      "Analyse HTML pour contenu article (avec sélecteur .reader-article-content)...\n",
      " Contenu trouvé via: Sélecteur 'div.reader-article-content'\n",
      "\n",
      "Recherche des Likes/Réactions...\n",
      " Likes trouvés via XPath aria-label: '//button[contains(@aria-label, 'réaction') or contains(@aria-label, 'reaction')]' -> 20\n",
      "\n",
      "Recherche des Commentaires...\n",
      " Commentaires trouvés via XPath aria-label: '//button[contains(@aria-label, 'commentaire') or contains(@aria-label, 'comment')]' -> 4\n",
      "-> Résultat ajouté pour https://fr.linkedin.com/pulse/lai-act-encadrer-lintelligence-artificielle-pour-un-futur-spitz-xks0e\n",
      "   Likes: 20, Commentaires: 4\n",
      "--- Pause de 17.8 secondes avant la prochaine URL ---\n",
      "\n",
      "------------------------------\n",
      "Traitement URL 7/10 : https://fr.linkedin.com/pulse/lia-g%C3%A9n%C3%A9rative-r%C3%A9volution-ou-simple-%C3%A9volution-maltem-africa-2ahae\n",
      "------------------------------\n",
      "\n",
      "Accès à l'URL cible via Selenium : https://fr.linkedin.com/pulse/lia-g%C3%A9n%C3%A9rative-r%C3%A9volution-ou-simple-%C3%A9volution-maltem-africa-2ahae\n",
      " Attente chargement (5-8 sec)...\n",
      " Défilement léger...\n",
      " Scroll 1/2...\n",
      " Scroll 2/2...\n",
      " Élément H1 visible.\n",
      "✅ Contenu HTML récupéré.\n",
      "\n",
      "Analyse HTML pour contenu article (avec sélecteur .reader-article-content)...\n",
      " Contenu trouvé via: Sélecteur 'div.reader-article-content'\n",
      "\n",
      "Recherche des Likes/Réactions...\n",
      " Likes trouvés via XPath aria-label: '//button[contains(@aria-label, 'réaction') or contains(@aria-label, 'reaction')]' -> 19\n",
      "\n",
      "Recherche des Commentaires...\n",
      "⚠️ Élément Commentaires non trouvé via XPath.\n",
      "⚠️ Nombre de commentaires non trouvé via sélecteurs principaux.\n",
      "-> Résultat ajouté pour https://fr.linkedin.com/pulse/lia-g%C3%A9n%C3%A9rative-r%C3%A9volution-ou-simple-%C3%A9volution-maltem-africa-2ahae\n",
      "   Likes: 19, Commentaires: Non trouvé\n",
      "--- Pause de 15.5 secondes avant la prochaine URL ---\n",
      "\n",
      "------------------------------\n",
      "Traitement URL 8/10 : https://fr.linkedin.com/pulse/lintelligence-artificielle-g%C3%A9n%C3%A9rative-un-nouveau-n8q6e\n",
      "------------------------------\n",
      "\n",
      "Accès à l'URL cible via Selenium : https://fr.linkedin.com/pulse/lintelligence-artificielle-g%C3%A9n%C3%A9rative-un-nouveau-n8q6e\n",
      " Attente chargement (5-8 sec)...\n",
      " Défilement léger...\n",
      " Scroll 1/2...\n",
      " Scroll 2/2...\n",
      " Élément H1 visible.\n",
      "✅ Contenu HTML récupéré.\n",
      "\n",
      "Analyse HTML pour contenu article (avec sélecteur .reader-article-content)...\n",
      " Contenu trouvé via: Sélecteur 'div.reader-article-content'\n",
      "\n",
      "Recherche des Likes/Réactions...\n",
      " Likes trouvés via XPath aria-label: '//button[contains(@aria-label, 'réaction') or contains(@aria-label, 'reaction')]' -> 3\n",
      "\n",
      "Recherche des Commentaires...\n",
      " Commentaires trouvés via XPath aria-label: '//button[contains(@aria-label, 'commentaire') or contains(@aria-label, 'comment')]' -> 2\n",
      "-> Résultat ajouté pour https://fr.linkedin.com/pulse/lintelligence-artificielle-g%C3%A9n%C3%A9rative-un-nouveau-n8q6e\n",
      "   Likes: 3, Commentaires: 2\n",
      "--- Pause de 19.4 secondes avant la prochaine URL ---\n",
      "\n",
      "------------------------------\n",
      "Traitement URL 9/10 : https://fr.linkedin.com/pulse/machine-learning-ia-vs-g%C3%A9n%C3%A9rative-comprendre-lessentiel-sarr-mtcne\n",
      "------------------------------\n",
      "\n",
      "Accès à l'URL cible via Selenium : https://fr.linkedin.com/pulse/machine-learning-ia-vs-g%C3%A9n%C3%A9rative-comprendre-lessentiel-sarr-mtcne\n",
      " Attente chargement (5-8 sec)...\n",
      " Défilement léger...\n",
      " Scroll 1/2...\n",
      " Scroll 2/2...\n",
      " Élément H1 visible.\n",
      "✅ Contenu HTML récupéré.\n",
      "\n",
      "Analyse HTML pour contenu article (avec sélecteur .reader-article-content)...\n",
      " Contenu trouvé via: Sélecteur 'div.reader-article-content'\n",
      "\n",
      "Recherche des Likes/Réactions...\n",
      " Likes trouvés via XPath aria-label: '//button[contains(@aria-label, 'réaction') or contains(@aria-label, 'reaction')]' -> 16\n",
      "\n",
      "Recherche des Commentaires...\n",
      " Commentaires trouvés via XPath aria-label: '//button[contains(@aria-label, 'commentaire') or contains(@aria-label, 'comment')]' -> 1\n",
      "-> Résultat ajouté pour https://fr.linkedin.com/pulse/machine-learning-ia-vs-g%C3%A9n%C3%A9rative-comprendre-lessentiel-sarr-mtcne\n",
      "   Likes: 16, Commentaires: 1\n",
      "--- Pause de 13.9 secondes avant la prochaine URL ---\n",
      "\n",
      "------------------------------\n",
      "Traitement URL 10/10 : https://fr.linkedin.com/pulse/machine-learning-llm-ia-g%C3%A9n%C3%A9rative-ce-que-ces-types-dia-ribeiro-aiyff\n",
      "------------------------------\n",
      "\n",
      "Accès à l'URL cible via Selenium : https://fr.linkedin.com/pulse/machine-learning-llm-ia-g%C3%A9n%C3%A9rative-ce-que-ces-types-dia-ribeiro-aiyff\n",
      " Attente chargement (5-8 sec)...\n",
      " Défilement léger...\n",
      " Scroll 1/2...\n",
      " Scroll 2/2...\n",
      " Élément H1 visible.\n",
      "✅ Contenu HTML récupéré.\n",
      "\n",
      "Analyse HTML pour contenu article (avec sélecteur .reader-article-content)...\n",
      " Contenu trouvé via: Sélecteur 'div.reader-article-content'\n",
      "\n",
      "Recherche des Likes/Réactions...\n",
      " Likes trouvés via XPath aria-label: '//button[contains(@aria-label, 'réaction') or contains(@aria-label, 'reaction')]' -> 10\n",
      "\n",
      "Recherche des Commentaires...\n",
      "⚠️ Élément Commentaires non trouvé via XPath.\n",
      "⚠️ Nombre de commentaires non trouvé via sélecteurs principaux.\n",
      "-> Résultat ajouté pour https://fr.linkedin.com/pulse/machine-learning-llm-ia-g%C3%A9n%C3%A9rative-ce-que-ces-types-dia-ribeiro-aiyff\n",
      "   Likes: 10, Commentaires: Non trouvé\n",
      "--- Pause de 14.4 secondes avant la prochaine URL ---\n",
      "\n",
      "✅ Navigateur Selenium fermé après la boucle.\n",
      "\n",
      "==================================================\n",
      "FIN DU SCRAPING : 10 URLs traitées.\n",
      "==================================================\n",
      "Tentative de sauvegarde des résultats dans linkedin_extracted_data.csv...\n",
      "✅ Résultats sauvegardés avec succès dans 'linkedin_extracted_data.csv' !\n"
     ]
    }
   ],
   "source": [
    "# --- Logique Principale d'Exécution (Boucle sur les URLs et Sauvegarde CSV) ---\n",
    "\n",
    "# Imports supplémentaires nécessaires\n",
    "import csv\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "# Assurez-vous que les variables LINKEDIN_USERNAME/PASSWORD sont chargées (Cellule 1)\n",
    "# Assurez-vous que les fonctions Selenium/Extraction sont définies (Cellule 2)\n",
    "\n",
    "# --- Configuration ---\n",
    "urls_filename = \"linkedin_urls_found.txt\"\n",
    "output_csv_filename = \"linkedin_extracted_data.csv\" # Nom du fichier de sortie CSV\n",
    "urls_to_process = [] # Liste des URLs à traiter\n",
    "all_results = [] # Liste pour stocker tous les dictionnaires de résultats\n",
    "driver = None # Initialiser\n",
    "\n",
    "# --- Charger TOUTES les URLs ---\n",
    "# Vérifier si le fichier existe dans le dossier courant (normalement 01_collecte_donnees)\n",
    "if os.path.exists(urls_filename):\n",
    "    try:\n",
    "        with open(urls_filename, \"r\") as f:\n",
    "            # Lire seulement les lignes valides contenant 'linkedin.com'\n",
    "            urls_to_process = [line.strip() for line in f if line.strip() and 'linkedin.com' in line]\n",
    "        if urls_to_process:\n",
    "            print(f\"✅ {len(urls_to_process)} URLs chargées depuis '{urls_filename}' pour traitement.\")\n",
    "            # Optionnel: Limiter le nombre pour tester la boucle rapidement\n",
    "            # urls_to_process = urls_to_process[:3] # Prend seulement les 3 premières\n",
    "            # print(f\" -> Limitation à {len(urls_to_process)} URLs pour ce test.\")\n",
    "        else:\n",
    "            print(f\"⚠️ Le fichier '{urls_filename}' est vide ou ne contient pas d'URLs valides.\")\n",
    "            urls_to_process = [] # Assurer liste vide\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors de la lecture du fichier URLs : {e}\")\n",
    "        urls_to_process = []\n",
    "else:\n",
    "    print(f\"❌ Fichier URLs '{urls_filename}' non trouvé.\")\n",
    "    urls_to_process = []\n",
    "\n",
    "# --- Exécuter seulement si on a des URLs et les identifiants ---\n",
    "if urls_to_process and 'LINKEDIN_USERNAME' in locals() and LINKEDIN_USERNAME and 'LINKEDIN_PASSWORD' in locals() and LINKEDIN_PASSWORD and 'setup_driver' in globals():\n",
    "\n",
    "    try:\n",
    "        # 1. Démarrer le navigateur UNE SEULE FOIS\n",
    "        print(\"\\n--- Démarrage du processus de scraping en boucle ---\")\n",
    "        driver = setup_driver() # Fonction définie en Cellule 2\n",
    "\n",
    "        if driver:\n",
    "            # 2. Se connecter UNE SEULE FOIS\n",
    "            login_success = linkedin_login(driver, LINKEDIN_USERNAME, LINKEDIN_PASSWORD) # Fonction définie en Cellule 2\n",
    "\n",
    "            if login_success:\n",
    "                # 3. Boucler sur chaque URL\n",
    "                for index, target_url in enumerate(urls_to_process):\n",
    "                    print(\"\\n\" + \"-\"*30)\n",
    "                    print(f\"Traitement URL {index + 1}/{len(urls_to_process)} : {target_url}\")\n",
    "                    print(\"-\"*30)\n",
    "\n",
    "                    # Initialiser les résultats pour cette URL\n",
    "                    article_content = \"Non extrait\"\n",
    "                    likes_count = \"Non extrait\"\n",
    "                    comments_count = \"Non extrait\"\n",
    "                    # Pas de date ici\n",
    "\n",
    "                    try:\n",
    "                        # Récupérer le HTML de la page cible\n",
    "                        html_content = get_page_content_with_selenium(target_url, driver) # Fonction définie en Cellule 2\n",
    "\n",
    "                        if html_content:\n",
    "                            # Extraire les informations\n",
    "                            article_content = extract_article_content(html_content) # Fonction définie en Cellule 2\n",
    "                            likes_count = extract_likes_count_with_selenium(driver) # Fonction définie en Cellule 2\n",
    "                            comments_count = extract_comments_count_with_selenium(driver) # Fonction définie en Cellule 2\n",
    "                        else:\n",
    "                            # Marquer toutes les infos comme erronées si HTML non récupéré\n",
    "                            error_msg = \"Erreur: HTML non récupéré\"\n",
    "                            article_content, likes_count, comments_count = error_msg, error_msg, error_msg\n",
    "\n",
    "                        # Stocker le résultat de cette URL dans un dictionnaire\n",
    "                        resultat_url = {\n",
    "                            \"url\": target_url,\n",
    "                            # \"date_publication\": publication_date, # Pas de date\n",
    "                            \"likes\": likes_count,\n",
    "                            \"commentaires\": comments_count,\n",
    "                            \"contenu\": article_content if isinstance(article_content, str) else f\"Erreur: {article_content}\"\n",
    "                        }\n",
    "                        # Ajouter ce dictionnaire à la liste globale\n",
    "                        all_results.append(resultat_url)\n",
    "                        print(f\"-> Résultat ajouté pour {target_url}\")\n",
    "                        print(f\"   Likes: {likes_count}, Commentaires: {comments_count}\")\n",
    "\n",
    "                    except Exception as e_url:\n",
    "                        # Gérer une erreur spécifique à cette URL sans arrêter la boucle\n",
    "                        print(f\"❌ Erreur majeure lors du traitement de l'URL {target_url}: {e_url}\")\n",
    "                        resultat_url = {\"url\": target_url, \"likes\": \"Erreur\", \"commentaires\": \"Erreur\", \"contenu\": f\"Erreur scraping: {e_url}\"}\n",
    "                        all_results.append(resultat_url)\n",
    "\n",
    "                    # Pause INDISPENSABLE entre chaque URL\n",
    "                    sleep_time = random.uniform(10, 20) # Pause de 10 à 20 secondes (ajustable)\n",
    "                    print(f\"--- Pause de {sleep_time:.1f} secondes avant la prochaine URL ---\")\n",
    "                    time.sleep(sleep_time)\n",
    "\n",
    "            else: # Si la connexion initiale a échoué\n",
    "                print(\"❌ Échec de la connexion LinkedIn initiale, arrêt du traitement des URLs.\")\n",
    "\n",
    "    except Exception as e_main:\n",
    "        print(f\"❌ Une erreur majeure est survenue lors de l'exécution principale de la boucle : {e_main}\")\n",
    "    finally:\n",
    "        # Fermer le navigateur UNE SEULE FOIS à la fin\n",
    "        if driver:\n",
    "            try:\n",
    "                driver.quit()\n",
    "                print(\"\\n✅ Navigateur Selenium fermé après la boucle.\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Erreur lors de la fermeture du navigateur: {e}\")\n",
    "\n",
    "    # --- 4. Sauvegarder tous les résultats collectés en CSV ---\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"FIN DU SCRAPING : {len(all_results)} URLs traitées.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    if all_results:\n",
    "        print(f\"Tentative de sauvegarde des résultats dans {output_csv_filename}...\")\n",
    "        try:\n",
    "            # Définir les noms des colonnes (l'ordre est important)\n",
    "            fieldnames = ['url', 'likes', 'commentaires', 'contenu'] # Sans la date\n",
    "            with open(output_csv_filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')\n",
    "                writer.writeheader() # Écrire l'en-tête\n",
    "                writer.writerows(all_results) # Écrire les données\n",
    "            print(f\"✅ Résultats sauvegardés avec succès dans '{output_csv_filename}' !\")\n",
    "        except Exception as e_save:\n",
    "            print(f\"❌ Erreur lors de la sauvegarde du fichier CSV : {e_save}\")\n",
    "    else:\n",
    "        print(\"Aucun résultat n'a été collecté pour la sauvegarde.\")\n",
    "\n",
    "# Gérer les cas où on ne peut pas démarrer\n",
    "elif not urls_to_process:\n",
    "    print(\"\\nAucune URL à traiter trouvée dans le fichier.\")\n",
    "elif not ('LINKEDIN_USERNAME' in locals() and LINKEDIN_USERNAME and 'LINKEDIN_PASSWORD' in locals() and LINKEDIN_PASSWORD):\n",
    "     print(\"\\nIdentifiants LinkedIn manquants.\")\n",
    "else:\n",
    "     print(\"\\nProblème inconnu avant démarrage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement des données depuis linkedin_extracted_data.csv...\n",
      "✅ 10 lignes chargées.\n",
      "\n",
      "Nettoyage des colonnes Likes/Commentaires...\n",
      "   2 valeur(s) 'commentaires' non numériques trouvées (seront mises à 0).\n",
      "\n",
      "Nettoyage de la colonne Contenu...\n",
      "   3 lignes supprimées à cause de contenu invalide/erreur.\n",
      "   7 lignes restantes avec contenu potentiellement valide.\n",
      "\n",
      "Statistiques des Likes/Commentaires (sur données nettoyées) :\n",
      "       likes_clean  commentaires_clean\n",
      "count     7.000000            7.000000\n",
      "mean     20.285714            1.714286\n",
      "std      26.010987            1.496026\n",
      "min       3.000000            0.000000\n",
      "25%       8.000000            0.500000\n",
      "50%      10.000000            2.000000\n",
      "75%      17.500000            2.500000\n",
      "max      78.000000            4.000000\n",
      "\n",
      "Filtrage des posts : Likes >= 10 ET Commentaires >= 2...\n",
      "✅ 1 posts correspondent aux critères de filtrage.\n",
      "\n",
      "Formatage des 1 posts filtrés en JSONL pour 'finetuning_data_final.jsonl'...\n",
      "✅ 1 posts formatés et sauvegardés dans 'finetuning_data_final.jsonl'.\n",
      "   Ce fichier est prêt pour le fine-tuning OpenAI !\n"
     ]
    }
   ],
   "source": [
    "# --- Phase 3 : Filtrage des Données et Formatage pour Fine-Tuning ---\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np # numpy est souvent une dépendance de pandas\n",
    "\n",
    "# --- Configuration ---\n",
    "input_csv_filename = \"linkedin_extracted_data.csv\" # Fichier CSV créé par Phase 2\n",
    "output_jsonl_filename = \"finetuning_data_final.jsonl\" # Fichier final pour OpenAI\n",
    "\n",
    "# --- 3a: Charger et Préparer les Données ---\n",
    "print(f\"Chargement des données depuis {input_csv_filename}...\")\n",
    "# Vérifier l'existence du fichier dans le dossier courant (normalement 01_collecte_donnees)\n",
    "if not os.path.exists(input_csv_filename):\n",
    "    print(f\"❌ ERREUR: Le fichier {input_csv_filename} n'existe pas. Exécutez d'abord le script de scraping (Phase 2 - Cellule 3 avec boucle).\")\n",
    "    df = pd.DataFrame() # Créer DataFrame vide pour éviter erreurs plus loin\n",
    "else:\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv_filename)\n",
    "        print(f\"✅ {len(df)} lignes chargées.\")\n",
    "\n",
    "        # Afficher les premières lignes et les types de données\n",
    "        # print(\"\\nAperçu des données brutes (5 premières lignes) :\")\n",
    "        # print(df.head())\n",
    "        # print(\"\\nTypes de données initiaux :\")\n",
    "        # print(df.info()) # Décommentez pour plus de détails\n",
    "\n",
    "        # --- Nettoyage Likes/Commentaires ---\n",
    "        print(\"\\nNettoyage des colonnes Likes/Commentaires...\")\n",
    "        # Convertir en numérique, mettre NaN si erreur\n",
    "        # Utiliser errors='coerce' transforme les erreurs en NaN (Not a Number)\n",
    "        df['likes_num'] = pd.to_numeric(df['likes'], errors='coerce')\n",
    "        df['commentaires_num'] = pd.to_numeric(df['commentaires'], errors='coerce')\n",
    "\n",
    "        # Optionnel: Afficher combien de lignes avaient des erreurs de conversion\n",
    "        likes_errors = df['likes_num'].isna().sum() - df['likes'].isna().sum() # Compte les nouvelles NaN créées par coerce\n",
    "        comments_errors = df['commentaires_num'].isna().sum() - df['commentaires'].isna().sum()\n",
    "        if likes_errors > 0: print(f\"   {likes_errors} valeur(s) 'likes' non numériques trouvées (seront mises à 0).\")\n",
    "        if comments_errors > 0: print(f\"   {comments_errors} valeur(s) 'commentaires' non numériques trouvées (seront mises à 0).\")\n",
    "\n",
    "        # Remplacer les NaN (erreurs de conversion ou valeurs manquantes initiales) par 0 et convertir en entier\n",
    "        df['likes_clean'] = df['likes_num'].fillna(0).astype(int)\n",
    "        df['commentaires_clean'] = df['commentaires_num'].fillna(0).astype(int)\n",
    "\n",
    "        # --- Nettoyage Contenu ---\n",
    "        print(\"\\nNettoyage de la colonne Contenu...\")\n",
    "        # Enlever les lignes où le contenu est manquant ou indique une erreur claire\n",
    "        initial_rows = len(df)\n",
    "        # S'assurer que 'contenu' est de type string avant d'utiliser .str\n",
    "        df['contenu'] = df['contenu'].astype(str)\n",
    "        # Exclure les lignes contenant les marqueurs d'erreur ou de contenu non trouvé\n",
    "        error_patterns = \"Erreur|Non extrait|Contenu non trouvé|HTML non récupéré\"\n",
    "        df = df[df['contenu'].notna() & (~df['contenu'].str.contains(error_patterns, na=False, case=False, regex=True))]\n",
    "        rows_after_content_clean = len(df)\n",
    "        print(f\"   {initial_rows - rows_after_content_clean} lignes supprimées à cause de contenu invalide/erreur.\")\n",
    "        print(f\"   {rows_after_content_clean} lignes restantes avec contenu potentiellement valide.\")\n",
    "\n",
    "        # Afficher un résumé statistique des colonnes numériques propres des lignes restantes\n",
    "        if not df.empty:\n",
    "             print(\"\\nStatistiques des Likes/Commentaires (sur données nettoyées) :\")\n",
    "             # Afficher les stats uniquement pour les lignes où le contenu est valide\n",
    "             print(df[['likes_clean', 'commentaires_clean']].describe())\n",
    "        else:\n",
    "             print(\"\\nDataFrame vide après nettoyage du contenu.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors du chargement ou nettoyage des données CSV : {e}\")\n",
    "        df = pd.DataFrame() # Assurer que df est vide si erreur\n",
    "\n",
    "# --- 3b & 3c: Définir Critères et Filtrer ---\n",
    "# Continuer seulement si le DataFrame df n'est pas vide après chargement/nettoyage\n",
    "if 'df' in locals() and not df.empty:\n",
    "    # --- !!! À ADAPTER PAR VOUS !!! ---\n",
    "    # Définissez ici vos critères pour un post \"pertinent\" / de \"qualité\"\n",
    "    # REGARDEZ LES STATS CI-DESSUS pour fixer des seuils réalistes pour VOTRE échantillon\n",
    "    # Mettez des valeurs basses pour être sûr d'avoir des résultats avec seulement 10 URLs initiales\n",
    "    SEUIL_MIN_LIKES = 10       # EXEMPLE : au moins 10 likes (à adapter !)\n",
    "    SEUIL_MIN_COMMENTAIRES = 2 # EXEMPLE : au moins 2 commentaires (à adapter !)\n",
    "    # ---------------------------------\n",
    "\n",
    "    print(f\"\\nFiltrage des posts : Likes >= {SEUIL_MIN_LIKES} ET Commentaires >= {SEUIL_MIN_COMMENTAIRES}...\")\n",
    "\n",
    "    df_filtered = df[\n",
    "        (df['likes_clean'] >= SEUIL_MIN_LIKES) &\n",
    "        (df['commentaires_clean'] >= SEUIL_MIN_COMMENTAIRES)\n",
    "    ].copy() # .copy() pour éviter les warnings\n",
    "\n",
    "    print(f\"✅ {len(df_filtered)} posts correspondent aux critères de filtrage.\")\n",
    "\n",
    "    if not df_filtered.empty:\n",
    "        # print(\"Aperçu des posts filtrés (index, likes, commentaires) :\")\n",
    "        # print(df_filtered[['likes_clean', 'commentaires_clean']].head()) # Décommentez pour voir\n",
    "\n",
    "        # --- 3d: Formater en JSONL pour OpenAI ---\n",
    "        print(f\"\\nFormatage des {len(df_filtered)} posts filtrés en JSONL pour '{output_jsonl_filename}'...\")\n",
    "\n",
    "        # Adaptez ce prompt système si vous voulez que le modèle apprenne un style particulier\n",
    "        system_prompt = \"Vous êtes un expert en IA rédigeant des posts pour LinkedIn dans un style professionnel, engageant et informatif.\"\n",
    "\n",
    "        lines_written = 0\n",
    "        try:\n",
    "            with open(output_jsonl_filename, 'w', encoding='utf-8') as f_out:\n",
    "                # Itérer sur le DataFrame filtré\n",
    "                for index, row in df_filtered.iterrows():\n",
    "                    post_content = str(row['contenu']) # Assurer que c'est une string\n",
    "\n",
    "                    # Ajouter une vérification de longueur minimale si souhaité\n",
    "                    if len(post_content) > 150: # EXEMPLE: ignorer posts très courts (parfois juste un titre)\n",
    "                        message = { \"messages\": [\n",
    "                                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                                {\"role\": \"user\", \"content\": \"\"}, # Prompt utilisateur vide si on fine-tune le style/sujet\n",
    "                                {\"role\": \"assistant\", \"content\": post_content} # Le post réel est la réponse désirée\n",
    "                            ] }\n",
    "                        # Écrire chaque message comme une ligne JSON\n",
    "                        f_out.write(json.dumps(message, ensure_ascii=False) + \"\\n\")\n",
    "                        lines_written += 1\n",
    "                    else:\n",
    "                        # Optionnel : Informer si un post est ignoré car trop court\n",
    "                        # print(f\"   Info: Post à l'index {index} ignoré car trop court (longueur {len(post_content)}).\")\n",
    "                        pass # Ne rien faire pour les posts trop courts\n",
    "\n",
    "\n",
    "            print(f\"✅ {lines_written} posts formatés et sauvegardés dans '{output_jsonl_filename}'.\")\n",
    "            if lines_written > 0:\n",
    "                 print(\"   Ce fichier est prêt pour le fine-tuning OpenAI !\")\n",
    "            elif len(df_filtered) > 0: # Si des posts ont été filtrés mais aucun écrit\n",
    "                 print(\"   Aucun post n'a été écrit (vérifiez le seuil de longueur minimale ou le contenu).\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erreur lors du formatage ou de la sauvegarde JSONL : {e}\")\n",
    "    # Fin de \"if not df_filtered.empty:\"\n",
    "    elif len(df) > 0: # Si le df initial n'était pas vide mais qu'aucun post n'a passé le filtre\n",
    "         print(\"\\nAucun post ne correspond aux critères de filtrage définis.\")\n",
    "# Fin de \"if 'df' in locals() and not df.empty:\"\n",
    "else:\n",
    "    print(\"\\nLe DataFrame est vide ou n'a pas pu être chargé/nettoyé, impossible de continuer la Phase 3.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3 : Filtrage des Données et Formatage - Résultats\n",
    "\n",
    "**Objectif :** Traiter le fichier CSV (`linkedin_extracted_data.csv`) contenant les données brutes scrapées en Phase 2, afin de sélectionner les posts les plus pertinents (basés sur l'engagement) et de les formater pour le fine-tuning OpenAI.\n",
    "\n",
    "**Actions Réalisées par le Script :**\n",
    "\n",
    "1.  **Chargement :** 10 lignes ont été chargées depuis `linkedin_extracted_data.csv`.\n",
    "2.  **Nettoyage Likes/Commentaires :** Les colonnes ont été converties en nombres (2 erreurs de conversion pour les commentaires ont été gérées et mises à 0).\n",
    "3.  **Nettoyage Contenu :** Les lignes contenant des erreurs de scraping ou un contenu invalide ont été supprimées (3 lignes enlevées). Il restait 7 posts avec du contenu potentiellement valide.\n",
    "4.  **Statistiques :** Des statistiques descriptives ont été calculées sur les 7 posts valides (Likes : max 75, moyenne ~20 ; Commentaires : max 4, moyenne ~1.7).\n",
    "5.  **Filtrage :** Un filtre a été appliqué pour ne garder que les posts avec **Likes >= 10 ET Commentaires >= 2** (basé sur les seuils définis dans le code).\n",
    "6.  **Résultat du Filtrage :** **1 post** correspondait à ces critères sur l'échantillon de 7 posts valides.\n",
    "7.  **Formatage :** Ce post unique a été formaté en JSONL selon la structure attendue par l'API OpenAI (messages système/utilisateur/assistant).\n",
    "8.  **Sauvegarde :** Le résultat formaté a été sauvegardé dans le fichier `finetuning_data_final.jsonl`.\n",
    "\n",
    "**Conclusion de la Phase 3 :**\n",
    "\n",
    "* Le pipeline complet (Chargement -> Nettoyage -> Filtrage -> Formatage) fonctionne correctement !\n",
    "* Le fichier `finetuning_data_final.jsonl` a été créé et est techniquement prêt à être utilisé pour un fine-tuning sur OpenAI.\n",
    "\n",
    "---\n",
    "\n",
    "**Prochaine Étape Stratégique : Passage à l'Échelle**\n",
    "\n",
    "Le processus est validé, mais le jeu de données final ne contient qu'un seul exemple, ce qui est insuffisant pour un fine-tuning efficace. La prochaine étape essentielle est de **retourner à la Phase 1 pour collecter une quantité beaucoup plus importante d'URLs pertinentes**, afin d'alimenter ce pipeline et générer un fichier `finetuning_data_final.jsonl` contenant plusieurs centaines (ou milliers) d'exemples de haute qualité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on essaie maintnenat de recuperer plus de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Démarrage de la recherche Google Multi-Requêtes ---\n",
      "Nombre de requêtes à exécuter : 7\n",
      "Limite de traitement par requête : 30\n",
      "Pause entre requêtes Google : 5.0 secondes\n",
      "Fichier de sortie : linkedin_urls_found.txt\n",
      "\n",
      "==============================\n",
      "Requête 1/7 : 'site:linkedin.com/pulse/ (\"intelligence artificielle\" OR \"IA générative\")'\n",
      "==============================\n",
      "\n",
      "❌ Une erreur est survenue pendant la recherche pour la requête 1 : search() got an unexpected keyword argument 'pause'\n",
      "   Cela peut être dû à un blocage temporaire de Google. Passage à la requête suivante...\n",
      "\n",
      "==============================\n",
      "Requête 2/7 : 'site:linkedin.com/pulse/ (\"machine learning\" OR \"deep learning\" applications)'\n",
      "==============================\n",
      "\n",
      "❌ Une erreur est survenue pendant la recherche pour la requête 2 : search() got an unexpected keyword argument 'pause'\n",
      "   Cela peut être dû à un blocage temporaire de Google. Passage à la requête suivante...\n",
      "\n",
      "==============================\n",
      "Requête 3/7 : 'site:linkedin.com/pulse/ (\"EU AI Act\" OR \"Acte IA Europe\" OR \"régulation IA\")'\n",
      "==============================\n",
      "\n",
      "❌ Une erreur est survenue pendant la recherche pour la requête 3 : search() got an unexpected keyword argument 'pause'\n",
      "   Cela peut être dû à un blocage temporaire de Google. Passage à la requête suivante...\n",
      "\n",
      "==============================\n",
      "Requête 4/7 : 'site:linkedin.com/pulse/ (\"prompt engineering\" OR \"ingénierie de prompt\" OR \"chatbot\")'\n",
      "==============================\n",
      "\n",
      "❌ Une erreur est survenue pendant la recherche pour la requête 4 : search() got an unexpected keyword argument 'pause'\n",
      "   Cela peut être dû à un blocage temporaire de Google. Passage à la requête suivante...\n",
      "\n",
      "==============================\n",
      "Requête 5/7 : 'site:linkedin.com/pulse/ (IA OR \"intelligence artificielle\" entreprise OR PME)'\n",
      "==============================\n",
      "\n",
      "❌ Une erreur est survenue pendant la recherche pour la requête 5 : search() got an unexpected keyword argument 'pause'\n",
      "   Cela peut être dû à un blocage temporaire de Google. Passage à la requête suivante...\n",
      "\n",
      "==============================\n",
      "Requête 6/7 : 'site:linkedin.com/pulse/ (éthique IA OR \"AI ethics\" OR \"biais algorithmique\")'\n",
      "==============================\n",
      "\n",
      "❌ Une erreur est survenue pendant la recherche pour la requête 6 : search() got an unexpected keyword argument 'pause'\n",
      "   Cela peut être dû à un blocage temporaire de Google. Passage à la requête suivante...\n",
      "\n",
      "==============================\n",
      "Requête 7/7 : 'site:linkedin.com/pulse/ (MLOps OR \"mise en production IA\")'\n",
      "==============================\n",
      "\n",
      "❌ Une erreur est survenue pendant la recherche pour la requête 7 : search() got an unexpected keyword argument 'pause'\n",
      "   Cela peut être dû à un blocage temporaire de Google. Passage à la requête suivante...\n",
      "\n",
      "==================================================\n",
      "RECHERCHE GOOGLE TERMINÉE\n",
      "--- Total URLs LinkedIn Uniques Trouvées (toutes requêtes confondues): 0 ---\n",
      "==================================================\n",
      "\n",
      "Aucune URL LinkedIn pertinente n'a été trouvée au total.\n",
      "\n",
      "--- Prochaine Étape : Relancer les cellules de Phase 2 (Scraping Selenium) et Phase 3 (Filtrage/Formatage) avec ce nouveau fichier d'URLs ---\n"
     ]
    }
   ],
   "source": [
    "# --- Phase 1 : Découverte d'URLs via Google Search (Version Multi-Requêtes) ---\n",
    "\n",
    "# Imports nécessaires\n",
    "from googlesearch import search\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# !!! MODIFIEZ ET COMPLÉTEZ CETTE LISTE SELON VOS BESOINS !!!\n",
    "# Ajoutez des mots-clés précis, des sujets spécifiques, des noms d'experts...\n",
    "search_queries = [\n",
    "    'site:linkedin.com/pulse/ (\"intelligence artificielle\" OR \"IA générative\")',\n",
    "    'site:linkedin.com/pulse/ (\"machine learning\" OR \"deep learning\" applications)',\n",
    "    'site:linkedin.com/pulse/ (\"EU AI Act\" OR \"Acte IA Europe\" OR \"régulation IA\")',\n",
    "    'site:linkedin.com/pulse/ (\"prompt engineering\" OR \"ingénierie de prompt\" OR \"chatbot\")',\n",
    "    'site:linkedin.com/pulse/ (IA OR \"intelligence artificielle\" entreprise OR PME)',\n",
    "    'site:linkedin.com/pulse/ (éthique IA OR \"AI ethics\" OR \"biais algorithmique\")',\n",
    "    'site:linkedin.com/pulse/ (MLOps OR \"mise en production IA\")',\n",
    "    # Ajoutez d'autres requêtes ici ! Exemple :\n",
    "    # 'site:linkedin.com/pulse/ (\"traitement langage naturel\" OR NLP)',\n",
    "    # 'site:linkedin.com/posts/ (\"intelligence artificielle\" OR \"IA générative\" OR \"machine learning\")', # Posts standards (moins fiable)\n",
    "]\n",
    "\n",
    "# Limite MANUELLE du nombre de résultats Google à traiter PAR REQUÊTE\n",
    "# Mettez une valeur plus haute pour tenter d'avoir plus de résultats par recherche,\n",
    "# même si la bibliothèque simple peut être limitée. Essayons 30.\n",
    "results_limit_per_query = 30\n",
    "# Délai entre les requêtes à Google (IMPORTANT)\n",
    "pause_duration = 5.0 # Garder une pause raisonnable (5 sec ou plus)\n",
    "\n",
    "output_filename = \"linkedin_urls_found.txt\" # Ce fichier sera ÉCRASÉ\n",
    "\n",
    "# --- Stockage des URLs ---\n",
    "all_found_urls = set() # Utilise un set pour éviter les doublons globaux\n",
    "\n",
    "print(f\"--- Démarrage de la recherche Google Multi-Requêtes ---\")\n",
    "print(f\"Nombre de requêtes à exécuter : {len(search_queries)}\")\n",
    "print(f\"Limite de traitement par requête : {results_limit_per_query}\")\n",
    "print(f\"Pause entre requêtes Google : {pause_duration} secondes\")\n",
    "print(f\"Fichier de sortie : {output_filename}\")\n",
    "\n",
    "# --- Boucle sur chaque requête ---\n",
    "for i, query in enumerate(search_queries):\n",
    "    print(\"\\n\" + \"=\"*30)\n",
    "    print(f\"Requête {i+1}/{len(search_queries)} : '{query}'\")\n",
    "    print(\"=\"*30)\n",
    "\n",
    "    processed_for_this_query = 0\n",
    "    try:\n",
    "        # Appel simplifié\n",
    "        for url in search(query, pause=pause_duration, num_results=results_limit_per_query): # Ajout de num_results ici\n",
    "            processed_for_this_query += 1\n",
    "            # Filtrer pour ne garder que les URLs LinkedIn pertinentes\n",
    "            if 'linkedin.com/' in url:\n",
    "                # Exclure les pages non désirées\n",
    "                excluded_paths = ['/groups/', '/jobs/', '/company/', '/school/', '/showcase/', '/talent/', '/legal/', '/directory/', '/feed/']\n",
    "                if not any(excluded in url for excluded in excluded_paths):\n",
    "                    if url not in all_found_urls:\n",
    "                      print(f\"  -> Trouvé ({processed_for_this_query}): {url}\")\n",
    "                      all_found_urls.add(url)\n",
    "\n",
    "            # Arrêter si on a traité assez de résultats pour CETTE requête\n",
    "            # Note: la bibliothèque peut s'arrêter avant si elle ne trouve plus rien\n",
    "            if processed_for_this_query >= results_limit_per_query:\n",
    "                print(f\" Limite de {results_limit_per_query} résultats Google traités pour cette requête.\")\n",
    "                break\n",
    "        # Petite pause supplémentaire entre les grosses requêtes\n",
    "        print(f\" Fin de la requête {i+1}. Pause avant la suivante...\")\n",
    "        time.sleep(random.uniform(5, 10)) # Pause plus longue entre les requêtes\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Une erreur est survenue pendant la recherche pour la requête {i+1} : {e}\")\n",
    "        print(\"   Cela peut être dû à un blocage temporaire de Google. Passage à la requête suivante...\")\n",
    "        time.sleep(random.uniform(10, 20)) # Pause plus longue en cas d'erreur\n",
    "        continue # Passer à la requête suivante\n",
    "\n",
    "# --- Afficher et Sauvegarder les résultats ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"RECHERCHE GOOGLE TERMINÉE\")\n",
    "print(f\"--- Total URLs LinkedIn Uniques Trouvées (toutes requêtes confondues): {len(all_found_urls)} ---\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if all_found_urls:\n",
    "    print(f\"\\nTentative de sauvegarde des {len(all_found_urls)} URLs dans {output_filename}...\")\n",
    "    try:\n",
    "        # Écraser l'ancien fichier avec les nouveaux résultats\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "            for url in sorted(list(all_found_urls)):\n",
    "                f.write(url + \"\\n\")\n",
    "        print(f\"✅ URLs sauvegardées avec succès dans '{output_filename}' !\")\n",
    "    except Exception as e_save:\n",
    "        print(f\"❌ Erreur lors de la sauvegarde du fichier URLs : {e_save}\")\n",
    "else:\n",
    "    print(\"\\nAucune URL LinkedIn pertinente n'a été trouvée au total.\")\n",
    "\n",
    "print(\"\\n--- Prochaine Étape : Relancer les cellules de Phase 2 (Scraping Selenium) et Phase 3 (Filtrage/Formatage) avec ce nouveau fichier d'URLs ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le defi majeur a ete de trouver une solution pour scrapper un volume important d'urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "je decide de tester via scrapping dog "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "malheureusement echec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "etape 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkedin_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
