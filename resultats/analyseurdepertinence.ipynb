{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: nltk in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (3.9.1)\n",
      "Requirement already satisfied: flask in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: click in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from flask) (3.0.6)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from flask) (3.1.6)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from flask) (1.8.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from flask) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from importlib-metadata>=3.6.0->flask) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from Jinja2>=3.1.2->flask) (2.1.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "--- Cellule 1 terminée: Bibliothèques installées ou déjà à jour. ---\n"
     ]
    }
   ],
   "source": [
    "# Étape 1: Installation des bibliothèques\n",
    "!pip install pandas nltk flask\n",
    "print(\"--- Cellule 1 terminée: Bibliothèques installées ou déjà à jour. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration du chemin NLTK...\n",
      "/Users/Nicolas/nltk_data est déjà dans le chemin de recherche NLTK.\n",
      "\n",
      "Vérification/Téléchargement forcé des ressources NLTK...\n",
      "  Tentative de suppression de l'ancien dossier pour 'punkt'...\n",
      "    Ancien dossier 'punkt' supprimé.\n",
      "  Téléchargement/Vérification de 'punkt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/Nicolas/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    'punkt' téléchargé/vérifié avec succès dans /Users/Nicolas/nltk_data.\n",
      "  Téléchargement/Vérification de 'vader_lexicon'...\n",
      "    'vader_lexicon' téléchargé/vérifié avec succès dans /Users/Nicolas/nltk_data.\n",
      "  Tentative de suppression de l'ancien dossier pour 'stopwords'...\n",
      "    Ancien dossier 'stopwords' supprimé.\n",
      "  Téléchargement/Vérification de 'stopwords'...\n",
      "    'stopwords' téléchargé/vérifié avec succès dans /Users/Nicolas/nltk_data.\n",
      "\n",
      "NLTK cherchera les données dans: ['/Users/Nicolas/nltk_data', '/Users/Nicolas/linkedin_env/nltk_data', '/Users/Nicolas/linkedin_env/share/nltk_data', '/Users/Nicolas/linkedin_env/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n",
      "--- Cellule 2 terminée: Configuration NLTK et téléchargement des ressources. ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Étape 2: Configuration et Téléchargement Forcé NLTK\n",
    "import nltk\n",
    "import os\n",
    "import shutil # Pour supprimer si besoin\n",
    "\n",
    "print(\"Configuration du chemin NLTK...\")\n",
    "# Chemin vers le dossier nltk_data dans votre répertoire personnel\n",
    "# (Vérifiez/adaptez 'Nicolas' si besoin)\n",
    "nltk_data_path = os.path.expanduser('~/nltk_data')\n",
    "\n",
    "# Ajouter ce chemin à NLTK s'il n'y est pas\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "    print(f\"Ajouté {nltk_data_path} au chemin de recherche NLTK.\")\n",
    "else:\n",
    "    print(f\"{nltk_data_path} est déjà dans le chemin de recherche NLTK.\")\n",
    "\n",
    "# Liste des ressources NLTK nécessaires\n",
    "resources = ['punkt', 'vader_lexicon', 'stopwords']\n",
    "\n",
    "print(\"\\nVérification/Téléchargement forcé des ressources NLTK...\")\n",
    "for resource in resources:\n",
    "    resource_path = os.path.join(nltk_data_path, 'corpora' if resource == 'stopwords' else 'tokenizers' if resource == 'punkt' else 'sentiment' if resource == 'vader_lexicon' else '')\n",
    "    # Pour punkt et vader_lexicon, on cible le sous-dossier spécifique pour la suppression\n",
    "    if resource == 'punkt':\n",
    "        specific_path = os.path.join(nltk_data_path, 'tokenizers/punkt')\n",
    "    elif resource == 'vader_lexicon':\n",
    "         specific_path = os.path.join(nltk_data_path, 'sentiment/vader_lexicon')\n",
    "    elif resource == 'stopwords':\n",
    "         specific_path = os.path.join(nltk_data_path, 'corpora/stopwords')\n",
    "    else:\n",
    "         specific_path = None\n",
    "\n",
    "    # Tenter de supprimer l'ancien dossier si pertinent\n",
    "    if specific_path and os.path.exists(specific_path):\n",
    "        print(f\"  Tentative de suppression de l'ancien dossier pour '{resource}'...\")\n",
    "        try:\n",
    "            shutil.rmtree(specific_path)\n",
    "            print(f\"    Ancien dossier '{resource}' supprimé.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Erreur lors de la suppression de l'ancien dossier '{resource}': {e}\")\n",
    "\n",
    "    # Tenter le téléchargement\n",
    "    print(f\"  Téléchargement/Vérification de '{resource}'...\")\n",
    "    try:\n",
    "        # On spécifie le dossier de téléchargement pour être sûr\n",
    "        nltk.download(resource, download_dir=nltk_data_path, force=True, raise_on_error=True)\n",
    "        print(f\"    '{resource}' téléchargé/vérifié avec succès dans {nltk_data_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ERREUR lors du téléchargement de '{resource}': {e}\")\n",
    "        print(f\"    Vérifiez votre connexion internet ou les permissions sur {nltk_data_path}.\")\n",
    "\n",
    "print(f\"\\nNLTK cherchera les données dans: {nltk.data.path}\")\n",
    "print(\"--- Cellule 2 terminée: Configuration NLTK et téléchargement des ressources. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialisation des composants...\n",
      "  Analyseur de sentiment (SentimentIntensityAnalyzer) initialisé.\n",
      "  Liste de mots vides (stopwords) chargée.\n",
      "--- Cellule 3 terminée: Bibliothèques importées et composants initialisés. ---\n"
     ]
    }
   ],
   "source": [
    "# Étape 3: Importations et Initialisations\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer # Importé ici\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "print(\"Initialisation des composants...\")\n",
    "# Initialiser l'analyseur de sentiment APRES avoir configuré NLTK\n",
    "try:\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    print(\"  Analyseur de sentiment (SentimentIntensityAnalyzer) initialisé.\")\n",
    "except LookupError as e:\n",
    "    print(f\"  ERREUR: Ressource NLTK manquante pour SentimentIntensityAnalyzer ({e}).\")\n",
    "    print(\"  Vérifiez l'exécution et les messages de la Cellule 2.\")\n",
    "except Exception as e:\n",
    "     print(f\"  Erreur inattendue lors de l'initialisation de SentimentIntensityAnalyzer: {e}\")\n",
    "\n",
    "# Définir les mots vides en français et ajouter des termes spécifiques à l'IA\n",
    "try:\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    stop_words.update(['lintelligence', 'artificielle', 'lia', 'ia', 'intelligence', 'artificielle']) # Mots vides ajoutés dans app.py\n",
    "    print(\"  Liste de mots vides (stopwords) chargée.\")\n",
    "except LookupError as e:\n",
    "     print(f\"  ERREUR: Ressource NLTK manquante pour stopwords ({e}).\")\n",
    "     print(\"  Vérifiez l'exécution et les messages de la Cellule 2.\")\n",
    "     stop_words = set() # Utiliser un set vide en cas d'erreur\n",
    "except Exception as e:\n",
    "     print(f\"  Erreur inattendue lors du chargement des stopwords: {e}\")\n",
    "     stop_words = set() # Utiliser un set vide en cas d'erreur\n",
    "\n",
    "\n",
    "print(\"--- Cellule 3 terminée: Bibliothèques importées et composants initialisés. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Définition des fonctions d'analyse...\n",
      "--- Cellule 4 terminée: Fonctions d'analyse définies. ---\n"
     ]
    }
   ],
   "source": [
    "# Étape 4: Définition des Fonctions d'Analyse\n",
    "\n",
    "print(\"Définition des fonctions d'analyse...\")\n",
    "\n",
    "# Fonction pour nettoyer le texte\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(f'[{string.punctuation}]', ' ', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "# Fonction pour analyser le sentiment\n",
    "def analyze_sentiment(text):\n",
    "    # Vérifie que sia existe et a été initialisé dans la Cellule 3\n",
    "    if isinstance(text, str) and 'sia' in globals() and isinstance(sia, SentimentIntensityAnalyzer):\n",
    "        try:\n",
    "            sentiment = sia.polarity_scores(text)\n",
    "            return sentiment\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur pendant l'analyse de sentiment: {e}\")\n",
    "            return {'compound': 0, 'neg': 0, 'neu': 1.0, 'pos': 0}\n",
    "    # Retour neutre si sia n'est pas prêt ou si le texte est invalide\n",
    "    return {'compound': 0, 'neg': 0, 'neu': 1.0, 'pos': 0}\n",
    "\n",
    "# Fonction pour extraire les mots-clés\n",
    "def extract_keywords(text, top_n=10):\n",
    "    if not isinstance(text, str) or 'stop_words' not in globals(): # Vérifie que stop_words existe\n",
    "        return []\n",
    "    clean = clean_text(text)\n",
    "    try:\n",
    "        words = word_tokenize(clean, language='french')\n",
    "        # Filtrer les mots vides et non alphabétiques\n",
    "        words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "        return Counter(words).most_common(top_n)\n",
    "    except LookupError as e:\n",
    "         print(f\"ERREUR: Ressource NLTK manquante pour word_tokenize ({e}). Vérifiez Cellule 2.\")\n",
    "         return [] # Retourner liste vide en cas d'erreur\n",
    "    except Exception as e:\n",
    "         print(f\"Erreur pendant l'extraction de mots-clés: {e}\")\n",
    "         return []\n",
    "\n",
    "\n",
    "# Fonction pour calculer le score d'engagement potentiel (inspirée de)\n",
    "def calculate_engagement_score(text):\n",
    "    if not isinstance(text, str) or not text: return 0\n",
    "    sentiment = analyze_sentiment(text)\n",
    "    longueur = len(text)\n",
    "    keywords = extract_keywords(text)\n",
    "\n",
    "    sentiment_positif = sentiment.get('pos', 0) # Utilise .get pour sécurité\n",
    "    sentiment_negatif = sentiment.get('neg', 0)\n",
    "    sentiment_factor = (sentiment_positif * 0.7) - (sentiment_negatif * 0.3) # Pondération vue dans app.py\n",
    "\n",
    "    length_factor = 0 # Inspiré par analyse de et code\n",
    "    if 500 <= longueur <= 1200: length_factor = 0.25\n",
    "    elif 100 <= longueur < 500: length_factor = 0.15\n",
    "    elif 1200 < longueur <= 1800: length_factor = 0.1\n",
    "\n",
    "    keyword_factor = min(len(keywords), 10) / 20 # Normalisation vue dans app.py\n",
    "\n",
    "    raw_score_component = sentiment_factor + length_factor + keyword_factor\n",
    "    score = 40 + raw_score_component * 50 # Mise à l'échelle indicative\n",
    "\n",
    "    return max(0, min(100, round(score, 2)))\n",
    "\n",
    "\n",
    "# Fonction pour générer des suggestions d'amélioration (inspirée de)\n",
    "def generate_suggestions(text, score):\n",
    "    if not isinstance(text, str): return [\"Texte invalide.\"]\n",
    "    suggestions = []\n",
    "    sentiment = analyze_sentiment(text)\n",
    "    longueur = len(text)\n",
    "    keywords = extract_keywords(text)\n",
    "\n",
    "    # Suggestions Sentiment\n",
    "    if sentiment.get('compound', 0) < 0: suggestions.append(\"Utilisez un ton plus positif pour augmenter l'engagement.\")\n",
    "    elif sentiment.get('compound', 0) < 0.1 and sentiment.get('pos', 0) < 0.1: suggestions.append(\"Le ton est très neutre. Envisagez d'ajouter un peu plus d'enthousiasme.\")\n",
    "\n",
    "    # Suggestions Longueur (cf.)\n",
    "    if longueur < 300: suggestions.append(\"Votre post est très court. Ajoutez plus de détails/contexte.\")\n",
    "    elif longueur > 1600: suggestions.append(\"Votre post est assez long. Pensez à le raccourcir ou mieux le structurer.\")\n",
    "\n",
    "    # Suggestions Mots-clés\n",
    "    if len(keywords) < 4: suggestions.append(\"Utilisez plus de mots-clés pertinents liés à l'IA.\")\n",
    "\n",
    "    # Suggestions Structure/Engagement (cf.)\n",
    "    if \"?\" not in text: suggestions.append(\"Ajoutez une question ouverte pour encourager l'interaction.\")\n",
    "    if \"#\" not in text: suggestions.append(\"N'oubliez pas d'inclure des hashtags pertinents.\")\n",
    "    if not any(emoji in text for emoji in [\"😊\", \"👍\", \"🚀\", \"💡\", \"🤖\", \"👉\", \"💪\", \"🔍\", \"✅\", \"⭐\", \"📊\", \"📈\"]): suggestions.append(\"Pensez à intégrer quelques émojis pertinents.\")\n",
    "\n",
    "    # Suggestion générale score\n",
    "    if score < 40: suggestions.append(\"Score bas. Revoyez ton, longueur, engagement (question, mots-clés).\")\n",
    "    elif score > 85 and not suggestions: suggestions.append(\"Excellent post, bien optimisé !\")\n",
    "    elif not suggestions and score < 70: suggestions.append(\"Post correct. Vérifiez clarté et appel à l'action.\")\n",
    "\n",
    "    return suggestions\n",
    "\n",
    "print(\"--- Cellule 4 terminée: Fonctions d'analyse définies. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancement du test d'analyse...\n",
      "--- Analyse du Post ---\n",
      "Texte d'entrée:\n",
      "\n",
      "🚀 Grande nouvelle ! Nous lançons un outil basé sur l'intelligence artificielle pour optimiser vos posts.\n",
      "Il analyse la pertinence et suggère des améliorations. L'IA change la donne pour le marketing de contenu.\n",
      "Qu'en pensez-vous ? Comment l'IA impacte-t-elle déjà votre travail ? 🤔\n",
      "#IA #MarketingDigital #Innovation #ContentMarketing #IntelligenceArtificielle\n",
      "\n",
      "\n",
      "ERREUR: Ressource NLTK manquante pour word_tokenize (\n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/french/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/Users/Nicolas/nltk_data'\n",
      "    - '/Users/Nicolas/linkedin_env/nltk_data'\n",
      "    - '/Users/Nicolas/linkedin_env/share/nltk_data'\n",
      "    - '/Users/Nicolas/linkedin_env/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "). Vérifiez Cellule 2.\n",
      "ERREUR: Ressource NLTK manquante pour word_tokenize (\n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/french/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/Users/Nicolas/nltk_data'\n",
      "    - '/Users/Nicolas/linkedin_env/nltk_data'\n",
      "    - '/Users/Nicolas/linkedin_env/share/nltk_data'\n",
      "    - '/Users/Nicolas/linkedin_env/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "). Vérifiez Cellule 2.\n",
      "ERREUR: Ressource NLTK manquante pour word_tokenize (\n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/french/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/Users/Nicolas/nltk_data'\n",
      "    - '/Users/Nicolas/linkedin_env/nltk_data'\n",
      "    - '/Users/Nicolas/linkedin_env/share/nltk_data'\n",
      "    - '/Users/Nicolas/linkedin_env/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "). Vérifiez Cellule 2.\n",
      "📊 Score d'Engagement Potentiel: 47.5/100\n",
      "\n",
      "😊 Analyse de Sentiment:\n",
      "  Positif: 0.00%\n",
      "  Neutre: 100.00%\n",
      "  Négatif: 0.00%\n",
      "  Score Composé VADER: 0.000\n",
      "\n",
      "🔑 Mots-clés Principaux:\n",
      "  - Aucun mot-clé pertinent trouvé (ou erreur NLTK?).\n",
      "\n",
      "💡 Suggestions d'Amélioration:\n",
      "  - Le ton est très neutre. Envisagez d'ajouter un peu plus d'enthousiasme.\n",
      "  - Utilisez plus de mots-clés pertinents liés à l'IA.\n",
      "\n",
      "📄 Statistiques:\n",
      "  - Longueur: 361 caractères\n",
      "  - Nombre de mots (approximatif): 49\n",
      "\n",
      "--- Cellule 5 terminée: Test d'analyse exécuté. ---\n"
     ]
    }
   ],
   "source": [
    "# Étape 5: Tester l'Analyseur\n",
    "\n",
    "print(\"Lancement du test d'analyse...\")\n",
    "\n",
    "# --- Votre Texte LinkedIn Ici ---\n",
    "texte_linkedin = \"\"\"\n",
    "🚀 Grande nouvelle ! Nous lançons un outil basé sur l'intelligence artificielle pour optimiser vos posts.\n",
    "Il analyse la pertinence et suggère des améliorations. L'IA change la donne pour le marketing de contenu.\n",
    "Qu'en pensez-vous ? Comment l'IA impacte-t-elle déjà votre travail ? 🤔\n",
    "#IA #MarketingDigital #Innovation #ContentMarketing #IntelligenceArtificielle\n",
    "\"\"\"\n",
    "# ---------------------------------\n",
    "\n",
    "print(f\"--- Analyse du Post ---\")\n",
    "print(f\"Texte d'entrée:\\n{texte_linkedin}\\n\")\n",
    "\n",
    "if texte_linkedin and isinstance(texte_linkedin, str):\n",
    "    # Exécuter les fonctions définies dans la Cellule 4\n",
    "    score = calculate_engagement_score(texte_linkedin)\n",
    "    sentiment = analyze_sentiment(texte_linkedin)\n",
    "    mots_cles = extract_keywords(texte_linkedin)\n",
    "    suggestions = generate_suggestions(texte_linkedin, score)\n",
    "\n",
    "    # Afficher les résultats\n",
    "    print(f\"📊 Score d'Engagement Potentiel: {score}/100\")\n",
    "    print(\"\\n😊 Analyse de Sentiment:\")\n",
    "    print(f\"  Positif: {sentiment.get('pos', 0)*100:.2f}%\")\n",
    "    print(f\"  Neutre: {sentiment.get('neu', 0)*100:.2f}%\")\n",
    "    print(f\"  Négatif: {sentiment.get('neg', 0)*100:.2f}%\")\n",
    "    print(f\"  Score Composé VADER: {sentiment.get('compound', 0):.3f}\")\n",
    "\n",
    "    print(\"\\n🔑 Mots-clés Principaux:\")\n",
    "    if mots_cles:\n",
    "        for mot, compte in mots_cles: print(f\"  - {mot} (x{compte})\")\n",
    "    else: print(\"  - Aucun mot-clé pertinent trouvé (ou erreur NLTK?).\")\n",
    "\n",
    "    print(\"\\n💡 Suggestions d'Amélioration:\")\n",
    "    if suggestions:\n",
    "        for suggestion in suggestions: print(f\"  - {suggestion}\")\n",
    "    else: print(\"  - Aucune suggestion spécifique.\")\n",
    "\n",
    "    print(\"\\n📄 Statistiques:\")\n",
    "    print(f\"  - Longueur: {len(texte_linkedin)} caractères\")\n",
    "    print(f\"  - Nombre de mots (approximatif): {len(texte_linkedin.split())}\")\n",
    "\n",
    "else:\n",
    "    print(\"Le texte fourni est vide ou invalide. Impossible de l'analyser.\")\n",
    "\n",
    "print(\"\\n--- Cellule 5 terminée: Test d'analyse exécuté. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ancien dossier punkt supprimé.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "# Trouver le dossier punkt\n",
    "punkt_path = os.path.expanduser(\"~/nltk_data/tokenizers/punkt\")\n",
    "\n",
    "# Supprimer le dossier s'il existe\n",
    "if os.path.exists(punkt_path):\n",
    "    shutil.rmtree(punkt_path)\n",
    "    print(\"✅ Ancien dossier punkt supprimé.\")\n",
    "else:\n",
    "    print(\"ℹ️ Aucun dossier punkt à supprimer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')  # téléchargement standard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cellule 1 terminée: Bibliothèques installées ou déjà à jour. ---\n"
     ]
    }
   ],
   "source": [
    "# Étape 1: Installation des bibliothèques\n",
    "!pip install -q pandas nltk flask\n",
    "print(\"--- Cellule 1 terminée: Bibliothèques installées ou déjà à jour. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration du chemin NLTK...\n",
      "/Users/Nicolas/nltk_data est déjà dans le chemin de recherche NLTK.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/Nicolas/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLTK cherchera les données dans: ['/Users/Nicolas/nltk_data', '/Users/Nicolas/linkedin_env/nltk_data', '/Users/Nicolas/linkedin_env/share/nltk_data', '/Users/Nicolas/linkedin_env/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n",
      "--- Cellule 2 terminée: Configuration NLTK OK. ---\n"
     ]
    }
   ],
   "source": [
    "# Étape 2: Configuration et Téléchargement Forcé NLTK\n",
    "import nltk\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"Configuration du chemin NLTK...\")\n",
    "nltk_data_path = os.path.expanduser('~/nltk_data')\n",
    "\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "    print(f\"Ajouté {nltk_data_path} au chemin de recherche NLTK.\")\n",
    "else:\n",
    "    print(f\"{nltk_data_path} est déjà dans le chemin de recherche NLTK.\")\n",
    "\n",
    "# Supprimer punkt corrompu si besoin\n",
    "punkt_path = os.path.join(nltk_data_path, 'tokenizers/punkt')\n",
    "if os.path.exists(punkt_path):\n",
    "    shutil.rmtree(punkt_path)\n",
    "    print(\"✅ Ancien dossier 'punkt' supprimé.\")\n",
    "\n",
    "# Re-télécharger les ressources nécessaires\n",
    "resources = ['punkt', 'stopwords', 'vader_lexicon']\n",
    "for res in resources:\n",
    "    nltk.download(res, download_dir=nltk_data_path, force=True, raise_on_error=True)\n",
    "print(f\"\\nNLTK cherchera les données dans: {nltk.data.path}\")\n",
    "print(\"--- Cellule 2 terminée: Configuration NLTK OK. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialisation des composants...\n",
      "  ✅ Analyseur de sentiment initialisé.\n",
      "  ✅ Stopwords français chargés.\n",
      "--- Cellule 3 terminée ---\n"
     ]
    }
   ],
   "source": [
    "# Étape 3: Importations et Initialisations\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "import re, string, json\n",
    "\n",
    "print(\"Initialisation des composants...\")\n",
    "\n",
    "try:\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    print(\"  ✅ Analyseur de sentiment initialisé.\")\n",
    "except Exception as e:\n",
    "    print(f\"  ❌ Erreur d'initialisation de SentimentIntensityAnalyzer: {e}\")\n",
    "\n",
    "try:\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    stop_words.update({'lintelligence', 'artificielle', 'lia', 'ia', 'intelligence'})\n",
    "    print(\"  ✅ Stopwords français chargés.\")\n",
    "except Exception as e:\n",
    "    stop_words = set()\n",
    "    print(f\"  ❌ Erreur chargement stopwords: {e}\")\n",
    "\n",
    "print(\"--- Cellule 3 terminée ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cellule 4 terminée: Fonctions définies. ---\n"
     ]
    }
   ],
   "source": [
    "# Étape 4: Fonctions d'Analyse\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n",
    "        text = re.sub(r\"\\d+\", \"\", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    if isinstance(text, str) and 'sia' in globals():\n",
    "        try:\n",
    "            return sia.polarity_scores(text)\n",
    "        except:\n",
    "            return {'compound': 0, 'neg': 0, 'neu': 1.0, 'pos': 0}\n",
    "    return {'compound': 0, 'neg': 0, 'neu': 1.0, 'pos': 0}\n",
    "\n",
    "def extract_keywords(text, top_n=10):\n",
    "    if not isinstance(text, str) or 'stop_words' not in globals():\n",
    "        return []\n",
    "    clean = clean_text(text)\n",
    "    try:\n",
    "        words = word_tokenize(clean)  # ✅ ne PAS mettre language='french'\n",
    "        words = [w.lower() for w in words if w.isalpha() and w.lower() not in stop_words]\n",
    "        return Counter(words).most_common(top_n)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def calculate_engagement_score(text):\n",
    "    if not isinstance(text, str): return 0\n",
    "    sentiment = analyze_sentiment(text)\n",
    "    longueur = len(text)\n",
    "    keywords = extract_keywords(text)\n",
    "\n",
    "    sentiment_score = sentiment.get('pos', 0)*0.7 - sentiment.get('neg', 0)*0.3\n",
    "    length_score = 0.25 if 500 <= longueur <= 1200 else 0.15 if longueur < 500 else 0.1 if longueur <= 1800 else 0\n",
    "    keyword_score = min(len(keywords), 10) / 20\n",
    "\n",
    "    score = 40 + (sentiment_score + length_score + keyword_score) * 50\n",
    "    return max(0, min(100, round(score, 2)))\n",
    "\n",
    "def generate_suggestions(text, score):\n",
    "    if not isinstance(text, str): return [\"Texte invalide.\"]\n",
    "    s = []\n",
    "    sentiment = analyze_sentiment(text)\n",
    "    longueur = len(text)\n",
    "    keywords = extract_keywords(text)\n",
    "\n",
    "    if sentiment['compound'] < 0: s.append(\"Utilisez un ton plus positif.\")\n",
    "    elif sentiment['compound'] < 0.1 and sentiment['pos'] < 0.1: s.append(\"Ajoutez un peu plus d'enthousiasme.\")\n",
    "\n",
    "    if longueur < 300: s.append(\"Ajoutez plus de détails.\")\n",
    "    elif longueur > 1600: s.append(\"Texte long, structurez ou raccourcissez.\")\n",
    "\n",
    "    if len(keywords) < 4: s.append(\"Ajoutez des mots-clés liés à l'IA.\")\n",
    "    if \"?\" not in text: s.append(\"Ajoutez une question pour favoriser l'engagement.\")\n",
    "    if \"#\" not in text: s.append(\"Ajoutez des hashtags.\")\n",
    "    if not any(e in text for e in [\"🚀\", \"🤖\", \"💡\", \"👉\", \"😊\", \"✅\", \"📊\", \"📈\"]): s.append(\"Ajoutez quelques émojis pour le dynamisme.\")\n",
    "\n",
    "    if score < 40: s.append(\"Score bas. Revoir ton, clarté, engagement.\")\n",
    "    elif score > 85 and not s: s.append(\"Excellent post, rien à redire !\")\n",
    "    elif not s: s.append(\"Post correct. Ajoutez peut-être un appel à l'action.\")\n",
    "\n",
    "    return s\n",
    "\n",
    "print(\"--- Cellule 4 terminée: Fonctions définies. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analyse du Post ---\n",
      "\n",
      "\n",
      "🚀 Grande nouvelle ! laéjà votre travail ? 🤔\n",
      "#IA #MarketingDigital je n'en vreiens pas ::::#Innovation #ContentMarketing #IntelligenceArtificielle\n",
      "\n",
      "\n",
      "📊 Score d'Engagement: 47.5/100\n",
      "\n",
      "😊 Sentiment : Positif 0.0%, Neutre 100.0%, Négatif 0.0%\n",
      "  Score VADER Composé : 0.00\n",
      "\n",
      "🔑 Mots-clés :\n",
      "\n",
      "💡 Suggestions :\n",
      "  - Ajoutez un peu plus d'enthousiasme.\n",
      "  - Ajoutez plus de détails.\n",
      "  - Ajoutez des mots-clés liés à l'IA.\n"
     ]
    }
   ],
   "source": [
    "# Étape 5: Test de l'analyseur\n",
    "texte_linkedin = \"\"\"\n",
    "🚀 Grande nouvelle ! laéjà votre travail ? 🤔\n",
    "#IA #MarketingDigital je n'en vreiens pas ::::#Innovation #ContentMarketing #IntelligenceArtificielle\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Analyse du Post ---\\n\")\n",
    "print(texte_linkedin)\n",
    "\n",
    "score = calculate_engagement_score(texte_linkedin)\n",
    "sentiment = analyze_sentiment(texte_linkedin)\n",
    "keywords = extract_keywords(texte_linkedin)\n",
    "suggestions = generate_suggestions(texte_linkedin, score)\n",
    "\n",
    "print(f\"\\n📊 Score d'Engagement: {score}/100\")\n",
    "print(f\"\\n😊 Sentiment : Positif {sentiment['pos']*100:.1f}%, Neutre {sentiment['neu']*100:.1f}%, Négatif {sentiment['neg']*100:.1f}%\")\n",
    "print(f\"  Score VADER Composé : {sentiment['compound']:.2f}\")\n",
    "\n",
    "print(\"\\n🔑 Mots-clés :\")\n",
    "for mot, n in keywords: print(f\"  - {mot} (x{n})\")\n",
    "\n",
    "print(\"\\n💡 Suggestions :\")\n",
    "for s in suggestions: print(f\"  - {s}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkedin_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
