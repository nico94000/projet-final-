{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: nltk in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (3.9.1)\n",
      "Requirement already satisfied: flask in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: click in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from flask) (3.0.6)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from flask) (3.1.6)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from flask) (1.8.2)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from flask) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from importlib-metadata>=3.6.0->flask) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from Jinja2>=3.1.2->flask) (2.1.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/Nicolas/linkedin_env/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "--- Cellule 1 termin√©e: Biblioth√®ques install√©es ou d√©j√† √† jour. ---\n"
     ]
    }
   ],
   "source": [
    "# √âtape 1: Installation des biblioth√®ques\n",
    "!pip install pandas nltk flask\n",
    "print(\"--- Cellule 1 termin√©e: Biblioth√®ques install√©es ou d√©j√† √† jour. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration du chemin NLTK...\n",
      "/Users/Nicolas/nltk_data est d√©j√† dans le chemin de recherche NLTK.\n",
      "\n",
      "V√©rification/T√©l√©chargement forc√© des ressources NLTK...\n",
      "  Tentative de suppression de l'ancien dossier pour 'punkt'...\n",
      "    Ancien dossier 'punkt' supprim√©.\n",
      "  T√©l√©chargement/V√©rification de 'punkt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/Nicolas/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    'punkt' t√©l√©charg√©/v√©rifi√© avec succ√®s dans /Users/Nicolas/nltk_data.\n",
      "  T√©l√©chargement/V√©rification de 'vader_lexicon'...\n",
      "    'vader_lexicon' t√©l√©charg√©/v√©rifi√© avec succ√®s dans /Users/Nicolas/nltk_data.\n",
      "  Tentative de suppression de l'ancien dossier pour 'stopwords'...\n",
      "    Ancien dossier 'stopwords' supprim√©.\n",
      "  T√©l√©chargement/V√©rification de 'stopwords'...\n",
      "    'stopwords' t√©l√©charg√©/v√©rifi√© avec succ√®s dans /Users/Nicolas/nltk_data.\n",
      "\n",
      "NLTK cherchera les donn√©es dans: ['/Users/Nicolas/nltk_data', '/Users/Nicolas/linkedin_env/nltk_data', '/Users/Nicolas/linkedin_env/share/nltk_data', '/Users/Nicolas/linkedin_env/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n",
      "--- Cellule 2 termin√©e: Configuration NLTK et t√©l√©chargement des ressources. ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# √âtape 2: Configuration et T√©l√©chargement Forc√© NLTK\n",
    "import nltk\n",
    "import os\n",
    "import shutil # Pour supprimer si besoin\n",
    "\n",
    "print(\"Configuration du chemin NLTK...\")\n",
    "# Chemin vers le dossier nltk_data dans votre r√©pertoire personnel\n",
    "# (V√©rifiez/adaptez 'Nicolas' si besoin)\n",
    "nltk_data_path = os.path.expanduser('~/nltk_data')\n",
    "\n",
    "# Ajouter ce chemin √† NLTK s'il n'y est pas\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "    print(f\"Ajout√© {nltk_data_path} au chemin de recherche NLTK.\")\n",
    "else:\n",
    "    print(f\"{nltk_data_path} est d√©j√† dans le chemin de recherche NLTK.\")\n",
    "\n",
    "# Liste des ressources NLTK n√©cessaires\n",
    "resources = ['punkt', 'vader_lexicon', 'stopwords']\n",
    "\n",
    "print(\"\\nV√©rification/T√©l√©chargement forc√© des ressources NLTK...\")\n",
    "for resource in resources:\n",
    "    resource_path = os.path.join(nltk_data_path, 'corpora' if resource == 'stopwords' else 'tokenizers' if resource == 'punkt' else 'sentiment' if resource == 'vader_lexicon' else '')\n",
    "    # Pour punkt et vader_lexicon, on cible le sous-dossier sp√©cifique pour la suppression\n",
    "    if resource == 'punkt':\n",
    "        specific_path = os.path.join(nltk_data_path, 'tokenizers/punkt')\n",
    "    elif resource == 'vader_lexicon':\n",
    "         specific_path = os.path.join(nltk_data_path, 'sentiment/vader_lexicon')\n",
    "    elif resource == 'stopwords':\n",
    "         specific_path = os.path.join(nltk_data_path, 'corpora/stopwords')\n",
    "    else:\n",
    "         specific_path = None\n",
    "\n",
    "    # Tenter de supprimer l'ancien dossier si pertinent\n",
    "    if specific_path and os.path.exists(specific_path):\n",
    "        print(f\"  Tentative de suppression de l'ancien dossier pour '{resource}'...\")\n",
    "        try:\n",
    "            shutil.rmtree(specific_path)\n",
    "            print(f\"    Ancien dossier '{resource}' supprim√©.\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Erreur lors de la suppression de l'ancien dossier '{resource}': {e}\")\n",
    "\n",
    "    # Tenter le t√©l√©chargement\n",
    "    print(f\"  T√©l√©chargement/V√©rification de '{resource}'...\")\n",
    "    try:\n",
    "        # On sp√©cifie le dossier de t√©l√©chargement pour √™tre s√ªr\n",
    "        nltk.download(resource, download_dir=nltk_data_path, force=True, raise_on_error=True)\n",
    "        print(f\"    '{resource}' t√©l√©charg√©/v√©rifi√© avec succ√®s dans {nltk_data_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"    ERREUR lors du t√©l√©chargement de '{resource}': {e}\")\n",
    "        print(f\"    V√©rifiez votre connexion internet ou les permissions sur {nltk_data_path}.\")\n",
    "\n",
    "print(f\"\\nNLTK cherchera les donn√©es dans: {nltk.data.path}\")\n",
    "print(\"--- Cellule 2 termin√©e: Configuration NLTK et t√©l√©chargement des ressources. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialisation des composants...\n",
      "  Analyseur de sentiment (SentimentIntensityAnalyzer) initialis√©.\n",
      "  Liste de mots vides (stopwords) charg√©e.\n",
      "--- Cellule 3 termin√©e: Biblioth√®ques import√©es et composants initialis√©s. ---\n"
     ]
    }
   ],
   "source": [
    "# √âtape 3: Importations et Initialisations\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer # Import√© ici\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "\n",
    "print(\"Initialisation des composants...\")\n",
    "# Initialiser l'analyseur de sentiment APRES avoir configur√© NLTK\n",
    "try:\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    print(\"  Analyseur de sentiment (SentimentIntensityAnalyzer) initialis√©.\")\n",
    "except LookupError as e:\n",
    "    print(f\"  ERREUR: Ressource NLTK manquante pour SentimentIntensityAnalyzer ({e}).\")\n",
    "    print(\"  V√©rifiez l'ex√©cution et les messages de la Cellule 2.\")\n",
    "except Exception as e:\n",
    "     print(f\"  Erreur inattendue lors de l'initialisation de SentimentIntensityAnalyzer: {e}\")\n",
    "\n",
    "# D√©finir les mots vides en fran√ßais et ajouter des termes sp√©cifiques √† l'IA\n",
    "try:\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    stop_words.update(['lintelligence', 'artificielle', 'lia', 'ia', 'intelligence', 'artificielle']) # Mots vides ajout√©s dans app.py\n",
    "    print(\"  Liste de mots vides (stopwords) charg√©e.\")\n",
    "except LookupError as e:\n",
    "     print(f\"  ERREUR: Ressource NLTK manquante pour stopwords ({e}).\")\n",
    "     print(\"  V√©rifiez l'ex√©cution et les messages de la Cellule 2.\")\n",
    "     stop_words = set() # Utiliser un set vide en cas d'erreur\n",
    "except Exception as e:\n",
    "     print(f\"  Erreur inattendue lors du chargement des stopwords: {e}\")\n",
    "     stop_words = set() # Utiliser un set vide en cas d'erreur\n",
    "\n",
    "\n",
    "print(\"--- Cellule 3 termin√©e: Biblioth√®ques import√©es et composants initialis√©s. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D√©finition des fonctions d'analyse...\n",
      "--- Cellule 4 termin√©e: Fonctions d'analyse d√©finies. ---\n"
     ]
    }
   ],
   "source": [
    "# √âtape 4: D√©finition des Fonctions d'Analyse\n",
    "\n",
    "print(\"D√©finition des fonctions d'analyse...\")\n",
    "\n",
    "# Fonction pour nettoyer le texte\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(f'[{string.punctuation}]', ' ', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "# Fonction pour analyser le sentiment\n",
    "def analyze_sentiment(text):\n",
    "    # V√©rifie que sia existe et a √©t√© initialis√© dans la Cellule 3\n",
    "    if isinstance(text, str) and 'sia' in globals() and isinstance(sia, SentimentIntensityAnalyzer):\n",
    "        try:\n",
    "            sentiment = sia.polarity_scores(text)\n",
    "            return sentiment\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur pendant l'analyse de sentiment: {e}\")\n",
    "            return {'compound': 0, 'neg': 0, 'neu': 1.0, 'pos': 0}\n",
    "    # Retour neutre si sia n'est pas pr√™t ou si le texte est invalide\n",
    "    return {'compound': 0, 'neg': 0, 'neu': 1.0, 'pos': 0}\n",
    "\n",
    "# Fonction pour extraire les mots-cl√©s\n",
    "def extract_keywords(text, top_n=10):\n",
    "    if not isinstance(text, str) or 'stop_words' not in globals(): # V√©rifie que stop_words existe\n",
    "        return []\n",
    "    clean = clean_text(text)\n",
    "    try:\n",
    "        words = word_tokenize(clean, language='french')\n",
    "        # Filtrer les mots vides et non alphab√©tiques\n",
    "        words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "        return Counter(words).most_common(top_n)\n",
    "    except LookupError as e:\n",
    "         print(f\"ERREUR: Ressource NLTK manquante pour word_tokenize ({e}). V√©rifiez Cellule 2.\")\n",
    "         return [] # Retourner liste vide en cas d'erreur\n",
    "    except Exception as e:\n",
    "         print(f\"Erreur pendant l'extraction de mots-cl√©s: {e}\")\n",
    "         return []\n",
    "\n",
    "\n",
    "# Fonction pour calculer le score d'engagement potentiel (inspir√©e de)\n",
    "def calculate_engagement_score(text):\n",
    "    if not isinstance(text, str) or not text: return 0\n",
    "    sentiment = analyze_sentiment(text)\n",
    "    longueur = len(text)\n",
    "    keywords = extract_keywords(text)\n",
    "\n",
    "    sentiment_positif = sentiment.get('pos', 0) # Utilise .get pour s√©curit√©\n",
    "    sentiment_negatif = sentiment.get('neg', 0)\n",
    "    sentiment_factor = (sentiment_positif * 0.7) - (sentiment_negatif * 0.3) # Pond√©ration vue dans app.py\n",
    "\n",
    "    length_factor = 0 # Inspir√© par analyse de et code\n",
    "    if 500 <= longueur <= 1200: length_factor = 0.25\n",
    "    elif 100 <= longueur < 500: length_factor = 0.15\n",
    "    elif 1200 < longueur <= 1800: length_factor = 0.1\n",
    "\n",
    "    keyword_factor = min(len(keywords), 10) / 20 # Normalisation vue dans app.py\n",
    "\n",
    "    raw_score_component = sentiment_factor + length_factor + keyword_factor\n",
    "    score = 40 + raw_score_component * 50 # Mise √† l'√©chelle indicative\n",
    "\n",
    "    return max(0, min(100, round(score, 2)))\n",
    "\n",
    "\n",
    "# Fonction pour g√©n√©rer des suggestions d'am√©lioration (inspir√©e de)\n",
    "def generate_suggestions(text, score):\n",
    "    if not isinstance(text, str): return [\"Texte invalide.\"]\n",
    "    suggestions = []\n",
    "    sentiment = analyze_sentiment(text)\n",
    "    longueur = len(text)\n",
    "    keywords = extract_keywords(text)\n",
    "\n",
    "    # Suggestions Sentiment\n",
    "    if sentiment.get('compound', 0) < 0: suggestions.append(\"Utilisez un ton plus positif pour augmenter l'engagement.\")\n",
    "    elif sentiment.get('compound', 0) < 0.1 and sentiment.get('pos', 0) < 0.1: suggestions.append(\"Le ton est tr√®s neutre. Envisagez d'ajouter un peu plus d'enthousiasme.\")\n",
    "\n",
    "    # Suggestions Longueur (cf.)\n",
    "    if longueur < 300: suggestions.append(\"Votre post est tr√®s court. Ajoutez plus de d√©tails/contexte.\")\n",
    "    elif longueur > 1600: suggestions.append(\"Votre post est assez long. Pensez √† le raccourcir ou mieux le structurer.\")\n",
    "\n",
    "    # Suggestions Mots-cl√©s\n",
    "    if len(keywords) < 4: suggestions.append(\"Utilisez plus de mots-cl√©s pertinents li√©s √† l'IA.\")\n",
    "\n",
    "    # Suggestions Structure/Engagement (cf.)\n",
    "    if \"?\" not in text: suggestions.append(\"Ajoutez une question ouverte pour encourager l'interaction.\")\n",
    "    if \"#\" not in text: suggestions.append(\"N'oubliez pas d'inclure des hashtags pertinents.\")\n",
    "    if not any(emoji in text for emoji in [\"üòä\", \"üëç\", \"üöÄ\", \"üí°\", \"ü§ñ\", \"üëâ\", \"üí™\", \"üîç\", \"‚úÖ\", \"‚≠ê\", \"üìä\", \"üìà\"]): suggestions.append(\"Pensez √† int√©grer quelques √©mojis pertinents.\")\n",
    "\n",
    "    # Suggestion g√©n√©rale score\n",
    "    if score < 40: suggestions.append(\"Score bas. Revoyez ton, longueur, engagement (question, mots-cl√©s).\")\n",
    "    elif score > 85 and not suggestions: suggestions.append(\"Excellent post, bien optimis√© !\")\n",
    "    elif not suggestions and score < 70: suggestions.append(\"Post correct. V√©rifiez clart√© et appel √† l'action.\")\n",
    "\n",
    "    return suggestions\n",
    "\n",
    "print(\"--- Cellule 4 termin√©e: Fonctions d'analyse d√©finies. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lancement du test d'analyse...\n",
      "--- Analyse du Post ---\n",
      "Texte d'entr√©e:\n",
      "\n",
      "üöÄ Grande nouvelle ! Nous lan√ßons un outil bas√© sur l'intelligence artificielle pour optimiser vos posts.\n",
      "Il analyse la pertinence et sugg√®re des am√©liorations. L'IA change la donne pour le marketing de contenu.\n",
      "Qu'en pensez-vous ? Comment l'IA impacte-t-elle d√©j√† votre travail ? ü§î\n",
      "#IA #MarketingDigital #Innovation #ContentMarketing #IntelligenceArtificielle\n",
      "\n",
      "\n",
      "ERREUR: Ressource NLTK manquante pour word_tokenize (\n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/french/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/Users/Nicolas/nltk_data'\n",
      "    - '/Users/Nicolas/linkedin_env/nltk_data'\n",
      "    - '/Users/Nicolas/linkedin_env/share/nltk_data'\n",
      "    - '/Users/Nicolas/linkedin_env/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "). V√©rifiez Cellule 2.\n",
      "ERREUR: Ressource NLTK manquante pour word_tokenize (\n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/french/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/Users/Nicolas/nltk_data'\n",
      "    - '/Users/Nicolas/linkedin_env/nltk_data'\n",
      "    - '/Users/Nicolas/linkedin_env/share/nltk_data'\n",
      "    - '/Users/Nicolas/linkedin_env/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "). V√©rifiez Cellule 2.\n",
      "ERREUR: Ressource NLTK manquante pour word_tokenize (\n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt_tab')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt_tab/french/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/Users/Nicolas/nltk_data'\n",
      "    - '/Users/Nicolas/linkedin_env/nltk_data'\n",
      "    - '/Users/Nicolas/linkedin_env/share/nltk_data'\n",
      "    - '/Users/Nicolas/linkedin_env/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "**********************************************************************\n",
      "). V√©rifiez Cellule 2.\n",
      "üìä Score d'Engagement Potentiel: 47.5/100\n",
      "\n",
      "üòä Analyse de Sentiment:\n",
      "  Positif: 0.00%\n",
      "  Neutre: 100.00%\n",
      "  N√©gatif: 0.00%\n",
      "  Score Compos√© VADER: 0.000\n",
      "\n",
      "üîë Mots-cl√©s Principaux:\n",
      "  - Aucun mot-cl√© pertinent trouv√© (ou erreur NLTK?).\n",
      "\n",
      "üí° Suggestions d'Am√©lioration:\n",
      "  - Le ton est tr√®s neutre. Envisagez d'ajouter un peu plus d'enthousiasme.\n",
      "  - Utilisez plus de mots-cl√©s pertinents li√©s √† l'IA.\n",
      "\n",
      "üìÑ Statistiques:\n",
      "  - Longueur: 361 caract√®res\n",
      "  - Nombre de mots (approximatif): 49\n",
      "\n",
      "--- Cellule 5 termin√©e: Test d'analyse ex√©cut√©. ---\n"
     ]
    }
   ],
   "source": [
    "# √âtape 5: Tester l'Analyseur\n",
    "\n",
    "print(\"Lancement du test d'analyse...\")\n",
    "\n",
    "# --- Votre Texte LinkedIn Ici ---\n",
    "texte_linkedin = \"\"\"\n",
    "üöÄ Grande nouvelle ! Nous lan√ßons un outil bas√© sur l'intelligence artificielle pour optimiser vos posts.\n",
    "Il analyse la pertinence et sugg√®re des am√©liorations. L'IA change la donne pour le marketing de contenu.\n",
    "Qu'en pensez-vous ? Comment l'IA impacte-t-elle d√©j√† votre travail ? ü§î\n",
    "#IA #MarketingDigital #Innovation #ContentMarketing #IntelligenceArtificielle\n",
    "\"\"\"\n",
    "# ---------------------------------\n",
    "\n",
    "print(f\"--- Analyse du Post ---\")\n",
    "print(f\"Texte d'entr√©e:\\n{texte_linkedin}\\n\")\n",
    "\n",
    "if texte_linkedin and isinstance(texte_linkedin, str):\n",
    "    # Ex√©cuter les fonctions d√©finies dans la Cellule 4\n",
    "    score = calculate_engagement_score(texte_linkedin)\n",
    "    sentiment = analyze_sentiment(texte_linkedin)\n",
    "    mots_cles = extract_keywords(texte_linkedin)\n",
    "    suggestions = generate_suggestions(texte_linkedin, score)\n",
    "\n",
    "    # Afficher les r√©sultats\n",
    "    print(f\"üìä Score d'Engagement Potentiel: {score}/100\")\n",
    "    print(\"\\nüòä Analyse de Sentiment:\")\n",
    "    print(f\"  Positif: {sentiment.get('pos', 0)*100:.2f}%\")\n",
    "    print(f\"  Neutre: {sentiment.get('neu', 0)*100:.2f}%\")\n",
    "    print(f\"  N√©gatif: {sentiment.get('neg', 0)*100:.2f}%\")\n",
    "    print(f\"  Score Compos√© VADER: {sentiment.get('compound', 0):.3f}\")\n",
    "\n",
    "    print(\"\\nüîë Mots-cl√©s Principaux:\")\n",
    "    if mots_cles:\n",
    "        for mot, compte in mots_cles: print(f\"  - {mot} (x{compte})\")\n",
    "    else: print(\"  - Aucun mot-cl√© pertinent trouv√© (ou erreur NLTK?).\")\n",
    "\n",
    "    print(\"\\nüí° Suggestions d'Am√©lioration:\")\n",
    "    if suggestions:\n",
    "        for suggestion in suggestions: print(f\"  - {suggestion}\")\n",
    "    else: print(\"  - Aucune suggestion sp√©cifique.\")\n",
    "\n",
    "    print(\"\\nüìÑ Statistiques:\")\n",
    "    print(f\"  - Longueur: {len(texte_linkedin)} caract√®res\")\n",
    "    print(f\"  - Nombre de mots (approximatif): {len(texte_linkedin.split())}\")\n",
    "\n",
    "else:\n",
    "    print(\"Le texte fourni est vide ou invalide. Impossible de l'analyser.\")\n",
    "\n",
    "print(\"\\n--- Cellule 5 termin√©e: Test d'analyse ex√©cut√©. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ancien dossier punkt supprim√©.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "# Trouver le dossier punkt\n",
    "punkt_path = os.path.expanduser(\"~/nltk_data/tokenizers/punkt\")\n",
    "\n",
    "# Supprimer le dossier s'il existe\n",
    "if os.path.exists(punkt_path):\n",
    "    shutil.rmtree(punkt_path)\n",
    "    print(\"‚úÖ Ancien dossier punkt supprim√©.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Aucun dossier punkt √† supprimer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')  # t√©l√©chargement standard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cellule 1 termin√©e: Biblioth√®ques install√©es ou d√©j√† √† jour. ---\n"
     ]
    }
   ],
   "source": [
    "# √âtape 1: Installation des biblioth√®ques\n",
    "!pip install -q pandas nltk flask\n",
    "print(\"--- Cellule 1 termin√©e: Biblioth√®ques install√©es ou d√©j√† √† jour. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration du chemin NLTK...\n",
      "/Users/Nicolas/nltk_data est d√©j√† dans le chemin de recherche NLTK.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Nicolas/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/Nicolas/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLTK cherchera les donn√©es dans: ['/Users/Nicolas/nltk_data', '/Users/Nicolas/linkedin_env/nltk_data', '/Users/Nicolas/linkedin_env/share/nltk_data', '/Users/Nicolas/linkedin_env/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n",
      "--- Cellule 2 termin√©e: Configuration NLTK OK. ---\n"
     ]
    }
   ],
   "source": [
    "# √âtape 2: Configuration et T√©l√©chargement Forc√© NLTK\n",
    "import nltk\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"Configuration du chemin NLTK...\")\n",
    "nltk_data_path = os.path.expanduser('~/nltk_data')\n",
    "\n",
    "if nltk_data_path not in nltk.data.path:\n",
    "    nltk.data.path.append(nltk_data_path)\n",
    "    print(f\"Ajout√© {nltk_data_path} au chemin de recherche NLTK.\")\n",
    "else:\n",
    "    print(f\"{nltk_data_path} est d√©j√† dans le chemin de recherche NLTK.\")\n",
    "\n",
    "# Supprimer punkt corrompu si besoin\n",
    "punkt_path = os.path.join(nltk_data_path, 'tokenizers/punkt')\n",
    "if os.path.exists(punkt_path):\n",
    "    shutil.rmtree(punkt_path)\n",
    "    print(\"‚úÖ Ancien dossier 'punkt' supprim√©.\")\n",
    "\n",
    "# Re-t√©l√©charger les ressources n√©cessaires\n",
    "resources = ['punkt', 'stopwords', 'vader_lexicon']\n",
    "for res in resources:\n",
    "    nltk.download(res, download_dir=nltk_data_path, force=True, raise_on_error=True)\n",
    "print(f\"\\nNLTK cherchera les donn√©es dans: {nltk.data.path}\")\n",
    "print(\"--- Cellule 2 termin√©e: Configuration NLTK OK. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialisation des composants...\n",
      "  ‚úÖ Analyseur de sentiment initialis√©.\n",
      "  ‚úÖ Stopwords fran√ßais charg√©s.\n",
      "--- Cellule 3 termin√©e ---\n"
     ]
    }
   ],
   "source": [
    "# √âtape 3: Importations et Initialisations\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "import re, string, json\n",
    "\n",
    "print(\"Initialisation des composants...\")\n",
    "\n",
    "try:\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    print(\"  ‚úÖ Analyseur de sentiment initialis√©.\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ùå Erreur d'initialisation de SentimentIntensityAnalyzer: {e}\")\n",
    "\n",
    "try:\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    stop_words.update({'lintelligence', 'artificielle', 'lia', 'ia', 'intelligence'})\n",
    "    print(\"  ‚úÖ Stopwords fran√ßais charg√©s.\")\n",
    "except Exception as e:\n",
    "    stop_words = set()\n",
    "    print(f\"  ‚ùå Erreur chargement stopwords: {e}\")\n",
    "\n",
    "print(\"--- Cellule 3 termin√©e ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Cellule 4 termin√©e: Fonctions d√©finies. ---\n"
     ]
    }
   ],
   "source": [
    "# √âtape 4: Fonctions d'Analyse\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        text = text.lower()\n",
    "        text = re.sub(f\"[{string.punctuation}]\", \" \", text)\n",
    "        text = re.sub(r\"\\d+\", \"\", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    if isinstance(text, str) and 'sia' in globals():\n",
    "        try:\n",
    "            return sia.polarity_scores(text)\n",
    "        except:\n",
    "            return {'compound': 0, 'neg': 0, 'neu': 1.0, 'pos': 0}\n",
    "    return {'compound': 0, 'neg': 0, 'neu': 1.0, 'pos': 0}\n",
    "\n",
    "def extract_keywords(text, top_n=10):\n",
    "    if not isinstance(text, str) or 'stop_words' not in globals():\n",
    "        return []\n",
    "    clean = clean_text(text)\n",
    "    try:\n",
    "        words = word_tokenize(clean)  # ‚úÖ ne PAS mettre language='french'\n",
    "        words = [w.lower() for w in words if w.isalpha() and w.lower() not in stop_words]\n",
    "        return Counter(words).most_common(top_n)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def calculate_engagement_score(text):\n",
    "    if not isinstance(text, str): return 0\n",
    "    sentiment = analyze_sentiment(text)\n",
    "    longueur = len(text)\n",
    "    keywords = extract_keywords(text)\n",
    "\n",
    "    sentiment_score = sentiment.get('pos', 0)*0.7 - sentiment.get('neg', 0)*0.3\n",
    "    length_score = 0.25 if 500 <= longueur <= 1200 else 0.15 if longueur < 500 else 0.1 if longueur <= 1800 else 0\n",
    "    keyword_score = min(len(keywords), 10) / 20\n",
    "\n",
    "    score = 40 + (sentiment_score + length_score + keyword_score) * 50\n",
    "    return max(0, min(100, round(score, 2)))\n",
    "\n",
    "def generate_suggestions(text, score):\n",
    "    if not isinstance(text, str): return [\"Texte invalide.\"]\n",
    "    s = []\n",
    "    sentiment = analyze_sentiment(text)\n",
    "    longueur = len(text)\n",
    "    keywords = extract_keywords(text)\n",
    "\n",
    "    if sentiment['compound'] < 0: s.append(\"Utilisez un ton plus positif.\")\n",
    "    elif sentiment['compound'] < 0.1 and sentiment['pos'] < 0.1: s.append(\"Ajoutez un peu plus d'enthousiasme.\")\n",
    "\n",
    "    if longueur < 300: s.append(\"Ajoutez plus de d√©tails.\")\n",
    "    elif longueur > 1600: s.append(\"Texte long, structurez ou raccourcissez.\")\n",
    "\n",
    "    if len(keywords) < 4: s.append(\"Ajoutez des mots-cl√©s li√©s √† l'IA.\")\n",
    "    if \"?\" not in text: s.append(\"Ajoutez une question pour favoriser l'engagement.\")\n",
    "    if \"#\" not in text: s.append(\"Ajoutez des hashtags.\")\n",
    "    if not any(e in text for e in [\"üöÄ\", \"ü§ñ\", \"üí°\", \"üëâ\", \"üòä\", \"‚úÖ\", \"üìä\", \"üìà\"]): s.append(\"Ajoutez quelques √©mojis pour le dynamisme.\")\n",
    "\n",
    "    if score < 40: s.append(\"Score bas. Revoir ton, clart√©, engagement.\")\n",
    "    elif score > 85 and not s: s.append(\"Excellent post, rien √† redire !\")\n",
    "    elif not s: s.append(\"Post correct. Ajoutez peut-√™tre un appel √† l'action.\")\n",
    "\n",
    "    return s\n",
    "\n",
    "print(\"--- Cellule 4 termin√©e: Fonctions d√©finies. ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analyse du Post ---\n",
      "\n",
      "\n",
      "üöÄ Grande nouvelle ! la√©j√† votre travail ? ü§î\n",
      "#IA #MarketingDigital je n'en vreiens pas ::::#Innovation #ContentMarketing #IntelligenceArtificielle\n",
      "\n",
      "\n",
      "üìä Score d'Engagement: 47.5/100\n",
      "\n",
      "üòä Sentiment : Positif 0.0%, Neutre 100.0%, N√©gatif 0.0%\n",
      "  Score VADER Compos√© : 0.00\n",
      "\n",
      "üîë Mots-cl√©s :\n",
      "\n",
      "üí° Suggestions :\n",
      "  - Ajoutez un peu plus d'enthousiasme.\n",
      "  - Ajoutez plus de d√©tails.\n",
      "  - Ajoutez des mots-cl√©s li√©s √† l'IA.\n"
     ]
    }
   ],
   "source": [
    "# √âtape 5: Test de l'analyseur\n",
    "texte_linkedin = \"\"\"\n",
    "üöÄ Grande nouvelle ! la√©j√† votre travail ? ü§î\n",
    "#IA #MarketingDigital je n'en vreiens pas ::::#Innovation #ContentMarketing #IntelligenceArtificielle\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Analyse du Post ---\\n\")\n",
    "print(texte_linkedin)\n",
    "\n",
    "score = calculate_engagement_score(texte_linkedin)\n",
    "sentiment = analyze_sentiment(texte_linkedin)\n",
    "keywords = extract_keywords(texte_linkedin)\n",
    "suggestions = generate_suggestions(texte_linkedin, score)\n",
    "\n",
    "print(f\"\\nüìä Score d'Engagement: {score}/100\")\n",
    "print(f\"\\nüòä Sentiment : Positif {sentiment['pos']*100:.1f}%, Neutre {sentiment['neu']*100:.1f}%, N√©gatif {sentiment['neg']*100:.1f}%\")\n",
    "print(f\"  Score VADER Compos√© : {sentiment['compound']:.2f}\")\n",
    "\n",
    "print(\"\\nüîë Mots-cl√©s :\")\n",
    "for mot, n in keywords: print(f\"  - {mot} (x{n})\")\n",
    "\n",
    "print(\"\\nüí° Suggestions :\")\n",
    "for s in suggestions: print(f\"  - {s}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linkedin_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
